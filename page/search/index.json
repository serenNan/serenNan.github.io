[{"content":"\rFFmpeg 介绍 FFmpeg 库 FFmpeg 一共包含 8 个库：\navcodec: 编解码（最重要的库）* avformat: 封装格式处理 * avfilter: 滤镜特效处理 avdevice: 各种设备的输入输出 avutil: 工具库（大部分库都需要这个库的支持）* postproc: 后加工 swresample: 音频采样数据格式转换 swscale: 视频像素数据格式转换 * 其中 * 表示本次课程中会涉及到的库\nFFmpeg 解码流程 主要步骤 主要函数 函数名 描述 avformat_open_input 打开输入文件并创建 AVFormatContext avformat_find_stream_info 获取流信息 avcodec_find_decoder 查找解码器 avcodec_alloc_context3 分配解码器上下文 avcodec_parameters_to_context 复制编解码器参数 avcodec_open2 打开解码器 av_read_frame 读取数据包 avcodec_send_packet 发送数据包到解码器 avcodec_receive_frame 接收解码后的帧 sws_scale 像素格式转换 fwrite 写入转换后的帧数据 av_packet_unref 释放数据包 avcodec_free_context 释放解码器上下文 avformat_close_input 关闭输入流 FFmpeg 解码的数据结构 AV 表示 Audio Video\nAVFormatContext: 用于处理封装格式的上下文，包含视频最外层的信息\nAVInputFormat: 输入格式\nAVStream: 是一个数组，包含多个流，但是一般就包含视频流和音频流，第 0 个是视频流，第 1 个是音频流\nAVCodecContext: 编解码器上下文\nAVCodec: 编解码器，指明编码器的类型（h.264之类的）\nAVPacket: 压缩编码后的数据包 AVFrame: 解码后的数据包\nTODO：弄个图 AVPacket 解码完为 AVFrame\nAVFormatContext 用于处理封装格式的上下文，包含视频最外层的信息\niformat: 输入视频的 AVInputFormat nb_streams: 输入视频的 AVStream 个数 streams: 输入视频的 AVStream 数组 duration: 输入视频的时长（以微秒为单位） bit_rate: 输入视频的码率 AVInputFormat 输入格式\nname: 输入视频的格式名称 long_name: 输入视频格式的长名称 extensions: 输入视频格式的扩展名 id: 输入视频格式的 ID 一些封装格式处理的接口函数 AVStream 是一个数组，包含多个流，但是一般就包含视频流和音频流，第 0 个是视频流，第 1 个是音频流\nid: 输入视频流的 ID codecpar: 输入视频流的 AVCodecContext time_base: 输入视频流的时间基 r_frame_rate: 输入视频流的帧率 time_base 是一个分数，表示时间基，用于将时间戳转换为实际时间。 r_frame_rate 是一个分数，表示帧率，用于计算帧间隔时间。\nAVCodecContext 编解码器上下文\ncodec：编解码器的 AVCodec width, height: 图像的宽高 pix_fmt: 图像的像素格式 sample_rate: 音频的采样率 channels: 音频的声道数 sample_fmt: 音频的采样格式 AVCodec 编解码器，指明编码器的类型（h.264之类的）\nname: 编解码器的名称 long_name: 编解码器的全称 type: 编解码器的类型（视频、音频等） id: 编解码器的 ID 一些编解码的接口函数 AVPacket 压缩编码后的数据包，理解成装 h264 数据的盒子\npts: 显示时间戳 dts: 解码时间戳 data: 压缩编码的数据 size: 数据的大小 stream_index: 所属的 AVStream （音频流还是视频流） AVFrame 解码后的数据包，理解成装 yuv 数据的盒子\ndata: 解码后的图像数据（音频采样数据） linesize: 对视频来说是图像中的一行像素的大小；对音频来说是整个音频帧的大小 width, height: 视频帧的宽和高 key_frame: 是否是关键帧 pict_type: 帧类型（I, B, P 帧） 补充小知识 解码后的数据为什么要经过 sws_scale 转换？ 解码后 YUV 数据格式的视频像素数据保存在 AVFrame 的 data[0]，data[1]，data[2]，但是这些像素值并不是连续存储的，每行有效像素之后存储的是无效像素。 以亮度 Y 数据为例，data[0] 中一共包含了 linesize[0] * height 个数据。但是出于优化等方面考虑，linesize[0] 可能大于 width 。因此需要使用 sws_scale 进行转换。\n转换后去除了无效数据，width 和 linesize[0] 就相等了。\n运行代码 代码和 CMakeLists.txt 这里代码和雷神视频中的代码略有不同，修改了很多：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 #include \u0026lt;stdio.h\u0026gt; #define __STDC_CONSTANT_MACROS #ifdef _WIN32 // Windows extern \u0026#34;C\u0026#34; { #include \u0026#34;libavcodec/avcodec.h\u0026#34; #include \u0026#34;libavformat/avformat.h\u0026#34; #include \u0026#34;libavutil/imgutils.h\u0026#34; #include \u0026#34;libswscale/swscale.h\u0026#34; }; #else // Linux... #ifdef __cplusplus extern \u0026#34;C\u0026#34; { #endif #include \u0026lt;libavcodec/avcodec.h\u0026gt; #include \u0026lt;libavformat/avformat.h\u0026gt; #include \u0026lt;libavutil/imgutils.h\u0026gt; #include \u0026lt;libswscale/swscale.h\u0026gt; #ifdef __cplusplus }; #endif #endif int main(int argc, char *argv[]) { AVFormatContext *pFormatCtx = NULL; int videoindex = -1; AVCodecContext *pCodecCtx = NULL; const AVCodec *pCodec = NULL; AVFrame *pFrame = NULL, *pFrameYUV = NULL; unsigned char *out_buffer = NULL; AVPacket *packet = NULL; int ret = 0; struct SwsContext *img_convert_ctx = NULL; char filepath[] = \u0026#34;../video/input.mkv\u0026#34;; FILE *fp_yuv = fopen(\u0026#34;output.yuv\u0026#34;, \u0026#34;wb+\u0026#34;); // 初始化FFmpeg库 avformat_network_init(); // 打开输入文件 if (avformat_open_input(\u0026amp;pFormatCtx, filepath, NULL, NULL) != 0) { printf(\u0026#34;Couldn\u0026#39;t open input stream.\\n\u0026#34;); return -1; } // 获取流信息 if (avformat_find_stream_info(pFormatCtx, NULL) \u0026lt; 0) { printf(\u0026#34;Couldn\u0026#39;t find stream information.\\n\u0026#34;); return -1; } printf(\u0026#34;时长：%ld\\n\u0026#34;, pFormatCtx-\u0026gt;duration); // 查找视频流 for (int i = 0; i \u0026lt; pFormatCtx-\u0026gt;nb_streams; i++) { if (pFormatCtx-\u0026gt;streams[i]-\u0026gt;codecpar-\u0026gt;codec_type == AVMEDIA_TYPE_VIDEO) { videoindex = i; break; } } if (videoindex == -1) { printf(\u0026#34;Didn\u0026#39;t find a video stream.\\n\u0026#34;); return -1; } // 获取解码器 pCodec = avcodec_find_decoder(pFormatCtx-\u0026gt;streams[videoindex]-\u0026gt;codecpar-\u0026gt;codec_id); if (pCodec == NULL) { printf(\u0026#34;Codec not found.\\n\u0026#34;); return -1; } // 创建解码器上下文 pCodecCtx = avcodec_alloc_context3(pCodec); if (!pCodecCtx) { printf(\u0026#34;Could not allocate video codec context\\n\u0026#34;); return -1; } // 复制流参数到解码器上下文 if (avcodec_parameters_to_context(pCodecCtx, pFormatCtx-\u0026gt;streams[videoindex]-\u0026gt;codecpar) \u0026lt; 0) { printf(\u0026#34;Could not copy codec parameters to context\\n\u0026#34;); return -1; } // 打开解码器 if (avcodec_open2(pCodecCtx, pCodec, NULL) \u0026lt; 0) { printf(\u0026#34;Could not open codec.\\n\u0026#34;); return -1; } pFrame = av_frame_alloc(); pFrameYUV = av_frame_alloc(); out_buffer = (unsigned char *)av_malloc( av_image_get_buffer_size(AV_PIX_FMT_YUV420P, pCodecCtx-\u0026gt;width, pCodecCtx-\u0026gt;height, 1)); av_image_fill_arrays(pFrameYUV-\u0026gt;data, pFrameYUV-\u0026gt;linesize, out_buffer, AV_PIX_FMT_YUV420P, pCodecCtx-\u0026gt;width, pCodecCtx-\u0026gt;height, 1); packet = av_packet_alloc(); // 输出文件信息 printf(\u0026#34;--------------- File Information ----------------\\n\u0026#34;); av_dump_format(pFormatCtx, 0, filepath, 0); printf(\u0026#34;-------------------------------------------------\\n\u0026#34;); img_convert_ctx = sws_getContext(pCodecCtx-\u0026gt;width, pCodecCtx-\u0026gt;height, pCodecCtx-\u0026gt;pix_fmt, pCodecCtx-\u0026gt;width, pCodecCtx-\u0026gt;height, AV_PIX_FMT_YUV420P, SWS_BICUBIC, NULL, NULL, NULL); while (av_read_frame(pFormatCtx, packet) \u0026gt;= 0) { if (packet-\u0026gt;stream_index == videoindex) { ret = avcodec_send_packet(pCodecCtx, packet); if (ret \u0026lt; 0) { printf(\u0026#34;Error sending a packet for decoding\\n\u0026#34;); return -1; } while (ret \u0026gt;= 0) { ret = avcodec_receive_frame(pCodecCtx, pFrame); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) break; else if (ret \u0026lt; 0) { printf(\u0026#34;Error during decoding\\n\u0026#34;); return -1; } sws_scale(img_convert_ctx, (const unsigned char *const *)pFrame-\u0026gt;data, pFrame-\u0026gt;linesize, 0, pCodecCtx-\u0026gt;height, pFrameYUV-\u0026gt;data, pFrameYUV-\u0026gt;linesize); int y_size = pCodecCtx-\u0026gt;width * pCodecCtx-\u0026gt;height; fwrite(pFrameYUV-\u0026gt;data[0], 1, y_size, fp_yuv); // Y fwrite(pFrameYUV-\u0026gt;data[1], 1, y_size / 4, fp_yuv); // U fwrite(pFrameYUV-\u0026gt;data[2], 1, y_size / 4, fp_yuv); // V printf(\u0026#34;Succeed to decode 1 frame!\\n\u0026#34;); } } av_packet_unref(packet); } // 刷新解码器 avcodec_send_packet(pCodecCtx, NULL); while (ret \u0026gt;= 0) { ret = avcodec_receive_frame(pCodecCtx, pFrame); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) break; else if (ret \u0026lt; 0) { printf(\u0026#34;Error during decoding\\n\u0026#34;); return -1; } sws_scale(img_convert_ctx, (const unsigned char *const *)pFrame-\u0026gt;data, pFrame-\u0026gt;linesize, 0, pCodecCtx-\u0026gt;height, pFrameYUV-\u0026gt;data, pFrameYUV-\u0026gt;linesize); int y_size = pCodecCtx-\u0026gt;width * pCodecCtx-\u0026gt;height; fwrite(pFrameYUV-\u0026gt;data[0], 1, y_size, fp_yuv); // Y fwrite(pFrameYUV-\u0026gt;data[1], 1, y_size / 4, fp_yuv); // U fwrite(pFrameYUV-\u0026gt;data[2], 1, y_size / 4, fp_yuv); // V printf(\u0026#34;Flush Decoder: Succeed to decode 1 frame!\\n\u0026#34;); } sws_freeContext(img_convert_ctx); fclose(fp_yuv); av_frame_free(\u0026amp;pFrameYUV); av_frame_free(\u0026amp;pFrame); av_packet_free(\u0026amp;packet); avcodec_free_context(\u0026amp;pCodecCtx); avformat_close_input(\u0026amp;pFormatCtx); return 0; } CMakeLists.txt:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 cmake_minimum_required(VERSION 3.10) project(MyProject VERSION 1.0) set(CMAKE_CXX_STANDARD 11) set(CMAKE_CXX_STANDARD_REQUIRED ON) set(CMAKE_BUILD_TYPE DEBUG) set(CMAKE_EXPORT_COMPILE_COMMANDS ON) # 设置 FFmpeg 路径 set(FFMPEG_DIR /usr/local/ffmpeg) # 添加 FFmpeg 头文件路径 include_directories(${FFMPEG_DIR}/include) # 添加 FFmpeg 库文件路径 link_directories(${FFMPEG_DIR}/lib) # 添加可执行文件 add_executable(main src/decoder.cpp) # 链接 FFmpeg 库 target_link_libraries(main avcodec avformat avutil swscale ) # 链接 ZLIB 库（如果需要） find_package(ZLIB REQUIRED) target_link_libraries(main ZLIB::ZLIB) 手动编译一遍：\n1 2 3 4 5 mkdir build cd build cmake .. make ./main 如果成功输出，则说明配置成功\n没成功的话要注意看一下源码的路径和播放视频的路径是否正确\n我这里源码放在 src/decoder.cpp，播放视频放在video/input.mkv\n调试 在左侧工具栏找到运行和调试工具\n点击创建 launch.json 文件，选择C++(GDB/LLDB)\n在launch.json文件中添加以下内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;(gdb) Launch\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cppdbg\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${workspaceFolder}/build/main\u0026#34;, \u0026#34;args\u0026#34;: [], \u0026#34;stopAtEntry\u0026#34;: false, \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}/build\u0026#34;, \u0026#34;externalConsole\u0026#34;: false, \u0026#34;MIMode\u0026#34;: \u0026#34;gdb\u0026#34;, \u0026#34;setupCommands\u0026#34;: [ { \u0026#34;description\u0026#34;: \u0026#34;Enable pretty-printing for gdb\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;-enable-pretty-printing\u0026#34;, \u0026#34;ignoreFailures\u0026#34;: true } ], \u0026#34;preLaunchTask\u0026#34;: \u0026#34;cmake-build-debug\u0026#34; } ] } 按下 f5 即可调试\n这里可能会遇到两个问题：\n在源码位置按下 f5 后，会提示找不到找不到你链接的库\n这里可能原因是没执行你的 CMakeLists.txt 文件，换成点击左侧工具栏的启动按键就行\n提示找不到输入文件，可以试试去掉 json 中 cwd 字段的 /build：\n1 \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}\u0026#34; 练习 获取解码前的 h264 文件 注意这里只获取MPEG-TS，AVI格式的文件，如果是别的文件，无法直接获取\n这样重新编码后，就可以获取到解码前的 h264 文件了。\n代码分析 打开文件 1 FILE *fp_h264 = fopen(\u0026#34;test264.h264\u0026#34;, \u0026#34;wb+\u0026#34;); 循环从媒体文件中读取一帧数据，并将其存储在 AVPacket 结构体中。 1 while (av_read_frame(pFormatCtx, packet) \u0026gt;= 0) 这个函数会读取下一个可用的数据包，无论是音频、视频还是其他类型的流 判断是否为视频帧 1 if (packet-\u0026gt;stream_index == videoindex) 获取数据 1 fwrite(packet-\u0026gt;data, 1, packet-\u0026gt;size, fp_h264); 关闭文件 1 fclose(test264.h264) 注意事项 输入文件一定要是 ts 或 avi 格式！\n因为 mp4 和 flv 格式需要解析 moov 结构，而 ts 和 avi 格式可以直接解析 h264 数据\n我就是一开始没注意，导致浪费了很多时间\n获取完数据记得关闭文件\n获取解码后的 yuv 文件 打开文件 1 FILE *fp_yuv = fopen(\u0026#34;testyuv.yuv\u0026#34;, \u0026#34;wb+\u0026#34;); 循环从媒体文件中读取一帧数据，并将其存储在 AVPacket 结构体中。 1 while (av_read_frame(pFormatCtx, packet) \u0026gt;= 0) 判断当前帧是否为视频流 1 if (packet-\u0026gt;stream_index == video_index) 解码一帧视频数据 1 avcodec_send_packet(pCodecCtx, packet); 这个函数会将 AVPacket 中的压缩数据发送给解码器进行解码 接收解码后的数据 1 2 ret == avcodec_receive_frame(pCodecCtx, pFrame); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) break; 这个函数会从解码器中接收解码后的原始数据，并存储在 AVFrame 结构体中 AVERROR(EAGAIN) 是FFmpeg库中的一个错误码，表示当前没有足够的数据可供解码，需要等待更多数据到来才能继续解码。这种情况通常发生在数据流尚未准备好或缓冲区为空时 AVERROR_EOF 表示已经到达数据流的末尾（End of File），没有更多的数据可供解码 处理解码后的数据 1 2 3 sws_scale(img_convert_ctx, (const unsigned char *const *)pFrame-\u0026gt;data, pFrame-\u0026gt;linesize, 0, pCodecCtx-\u0026gt;height, pFrameYUV-\u0026gt;data, pFrameYUV-\u0026gt;linesize); 前面补充知识有提到， sws_scale 是FFmpeg中用于图像格式转换和缩放的函数。它可以将解码后的图像数据从一种格式转换为另一种格式，并可以进行缩放操作\n","date":"2025-02-18T00:00:00Z","image":"https://example.com/post/leixiaohua-note-1/cover.jpg","permalink":"https://example.com/post/leixiaohua-note-1/","title":"【雷霄骅课程笔记】1"},{"content":"图片加载问题 问题描述 有时会遇到在本地图片能成功加载，但部署到服务器上却不能成功显示。\n我在弄代码折叠图片时遇到了这个问题\n在 html 文件上加载图片的代码语句是\n1 img.src = \u0026#39;{{ (resources.Get \u0026#34;img/codeMore.png\u0026#34;).Permalink }}\u0026#39;; 在本地成功加载\n但是到发布到服务端就不行了\n根本问题是服务端的图片加载路径出错\n按 f12 调出开发者工具查看（或者右键对应位置，点击检查，能快速跳转），会发现图片加载路径不是图片相对路径\n解决方案 将加载图片的代码语句改成\n1 img.src = \u0026#39;{{ (resources.Get \u0026#34;img/codeMore.png\u0026#34;).RelPermalink }}\u0026#39;; 把最后的 Permalink 改成 RelPermalink\n重新加载，会发现服务端正确显示\n重新用开发者工具检查加载路径\n正确显示图片相对位置\n","date":"2025-02-10T00:00:00Z","image":"https://example.com/post/%E9%93%BE%E6%8E%A5/cover.jpg","permalink":"https://example.com/post/%E9%93%BE%E6%8E%A5/","title":"【Hugo】常见问题"},{"content":"这里的配置教程是另一处的，不是雷神的\n雷神的课程主要使用 Visual Studio 进行演示，但由于我个人不太习惯使用 VS，因此我选择了在 WSL 环境下使用 VSCode 和 CMake 来进行学习和实践\nwindows 环境总是出现各种不必要的麻烦，而且其实 linux 环境对音视频开发者更友好\n配置过程些许复杂，但只要按照步骤来，不会有太大问题，有问题可以留言，我会尽力解答\n我使用的资料都是比较新的（2025年2月），所以博客的一些代码和流程什么的可能和雷神介绍的有些不同\nffmpeg配置 前提准备 安装好 Ubuntu 和 VSCode, 并且配置好 WSL 和 VSCode 的连接\n这里网上很多教程，就不多说了\n编译安装 创建安装目录：\n1 sudo mkdir -p /usr/local/ffmpeg 下载 FFmpeg 源码： 我这里都是用最新的 FFmpeg 源码\n点击链接：下载地址\n目前最新版是 7.1 ，而且我是 ubuntu 环境\n下载压缩包之后进行解压：（具体解压指令可以问问 ai ）\n1 tar -xv ffmpeg-7.1.tar.xz 解压完之后要进入目录：\n1 cd ffmpeg-7.1 然后进行配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ./configure \\ 0 (20.192s) \u0026lt; 14:23:34 --prefix=\u0026#39;/usr/local/ffmpeg\u0026#39; \\ --enable-gpl \\ --enable-nonfree \\ --enable-ffplay \\ --enable-libfdk-aac \\ --enable-libmp3lame \\ --enable-libx264 \\ --enable-libx265 \\ --enable-filter=delogo \\ --enable-debug \\ --disable-optimizations \\ --enable-libspeex \\ --enable-shared \\ --enable-pthreads \\ --enable-version3 \\ --enable-hardcoded-tables \\ --extra-cflags=\u0026#34;-I/usr/local/ffmpeg/include\u0026#34; \\ --extra-ldflags=\u0026#34;-L/usr/local/ffmpeg/lib\u0026#34; 这里可能会不断报错，显示缺各种库，那根据提示去安装对应库就行\n下载：\n1 2 3 4 sudo apt-get install libasound2-dev sudo apt-get install libpulse-dev sudo apt-get install libx11-dev sudo apt-get install xorg-dev 配置：\n1 ./configure --prefix=/usr/local/ffmpeg --enable-shared --enable-video-x11 --enable-x11-shared --enable-video-x11-vm ffplay 一直无法播放视频的话（因为我是用的wsl2），尝试在终端配置文件加上 SDL_RENDER_DRIVER=software\n在 fish 终端配置文件加\n1 set -x SDL_RENDER_DRIVER software 具体怎么加可以问问 ai\nCMake配置 安装 CMake 安装插件 CMake Tools\n书写 CMakeLists.txt 在 CMakeLists.txt 下写入以下内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # 指定 CMake 的最低版本要求 cmake_minimum_required(VERSION 3.10) # 设置项目名称和版本 project(MyProject VERSION 1.0) # 设置 C++ 标准 set(CMAKE_CXX_STANDARD 11) set(CMAKE_CXX_STANDARD_REQUIRED ON) # 依赖 compile_commands.json 文件来理解项目的编译环境 set(CMAKE_EXPORT_COMPILE_COMMANDS ON) # 设置 FFmpeg 库的路径 set(FFmpeg_DIR /usr/local/ffmpeg) # 添加项目中的头文件目录 include_directories(${FFmpeg_DIR}/include) # 添加库文件目录 link_directories(${FFmpeg_DIR}/lib) # 添加可执行文件（就是源码的位置） add_executable(main src/testffmpeg.cpp) # 链接 FFmpeg 库 target_link_libraries(main avcodec avformat avutil) 测试代码 1 2 3 4 5 6 7 8 9 10 11 12 #define __STDC_CONSTANT_MACROS extern \u0026#34;C\u0026#34; { #include \u0026#34;libavcodec/avcodec.h\u0026#34; }; int main(int argc, char *argv[]) { printf(\u0026#34;%s\u0026#34;, avcodec_configuration()); return 0; } 编译 输入指令\n1 2 3 4 cd build cmake .. make ./main 如果成功输出，说明配置成功\n常见问题 解释器 如果成功输出了发现源文件仍然显示找不到头文件，可能是没安装好解释器，我使用的是 clangd，linux 环境安装很简单，问一下 ai 就行\n安装完成之后，clangd 会依赖 compile_commands.json 文件来理解项目的编译环境\n","date":"2025-02-10T00:00:00Z","image":"https://example.com/post/leixiaohua-note-0/cover.jpg","permalink":"https://example.com/post/leixiaohua-note-0/","title":"【雷霄骅课程笔记】0 配置"},{"content":"哈希表 map map是 C++ 标准库中的一个关联容器，用于存储键值对，键是唯一的，且按键的升序排列。\nm[key] = value：将键 key 对应的值设置为 value。 m.insert({key, value})：将键值对 {key, value} 插入到 map 中。 m.erase(key)：从 map 中删除键为 key 的元素。 m.count(key)：返回 map 中键为 key 的元素个数。 m.find(key)：返回指向 map 中键为 key 的元素的迭代器，如果 key 不存在则返回 m.end()。 m.lower_bound(key)：返回指向 map 中第一个不小于 key 的元素的迭代器。 m.upper_bound(key)：返回指向 map 中第一个大于 key 的元素的迭代器。 unordered_map unordered_map 是 C++ 标准库中的一个关联容器，用于存储键值对，键是唯一的。\numap[key] = value：将键 key 对应的值设置为 value。 umap.insert({key, value})：将键值对 {key, value} 插入到 unordered_map 中。 umap.erase(key)：从 unordered_map 中删除键为 key 的元素。 umap.count(key)：返回 unordered_map 中键为 key 的元素个数。 umap.find(key)：返回指向 unordered_map 中键为 key 的元素的迭代器，如果 key 不存在则返回 umap.end()。 umap.size()：返回 unordered_map 中元素的个数。 set set 是 C++ 标准库中的一个关联容器，用于存储唯一的元素。\n1 set\u0026lt;int\u0026gt; nums; nums.insert(x)：将元素 x 插入到集合 nums 中。 nums.erase(x)：从集合 nums 中删除元素 x。 nums.count(x)：返回集合 nums 中元素 x 的个数。 nums.find(x)：返回指向集合 nums 中元素 x 的迭代器，如果 x 不存在则返回 nums.end()。 nums.lower_bound(x)：返回指向集合 nums 中第一个不小于 x 的元素的迭代器。 emplace_back emplace_back 是 C++ 标准库中 std::vector、std::deque、std::list 等容器的一个成员函数，用于在容器的末尾直接构造一个元素，而不是先创建一个临时对象再插入。这样可以避免不必要的拷贝或移动操作，提高效率。\nqueue queue 是 C++ 标准库中的一个容器适配器，用于实现先进先出（FIFO）的队列。\n1 queue\u0026lt;int\u0026gt; q; q.push(x)：将元素 x 入队。 q.pop()：将队首元素出队。 q.front()：返回队首元素的引用。 q.back()：返回队尾元素的引用。 q.empty()：检查队列是否为空，返回布尔值。 q.size()：返回队列中元素的个数。 priority_queue priority_queue 是 C++ 标准库中的一个容器适配器，用于实现优先队列。\n1 priority_queue\u0026lt;int\u0026gt; pq; pq.push(x)：将元素 x 入队，并按优先级排序。 pq.pop()：将优先级最高的元素出队。 pq.top()：返回优先级最高的元素的引用。 pq.empty()：检查优先队列是否为空，返回布尔值。 pq.size()：返回优先队列中元素的个数。 stack stack 是 C++ 标准库中的一个容器适配器，用于实现后进先出（LIFO）的栈。\n1 stack\u0026lt;int\u0026gt; s; s.push(x)：将元素 x 压入栈顶。 s.pop()：弹出栈顶元素。 s.top()：返回栈顶元素的引用。 s.empty()：检查栈是否为空，返回布尔值。 s.size()：返回栈中元素的个数。 ","date":"2025-01-10T00:00:00Z","image":"https://example.com/post/function/function.jpg","permalink":"https://example.com/post/function/","title":"常用函数"},{"content":"前缀和 前缀和的定义： 给定数组 nums，我们可以定义前缀和数组 prefix_sum，使得 prefix_sum[i] = nums[0] + nums[1] + ... + nums[i-1]。 这样，对于任何子数组 [l, r]，其和可以通过以下公式计算：\n1 sum(l, r) = prefix_sum[r + 1] - prefix_sum[l] 典型题 437. 路径总和 III\n560. 和为 K 的子数组\n滑动窗口 滑动窗口\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 /* 滑动窗口算法框架 */ void slidingWindow(string s, string t) { unordered_map\u0026lt;char, int\u0026gt; need, window; for (char c : t) need[c]++; int left = 0, right = 0; int valid = 0; while (right \u0026lt; s.size()) { // c 是将移入窗口的字符 char c = s[right]; // 右移窗口 right++; // 进行窗口内数据的一系列更新 ... /*** debug 输出的位置 ***/ printf(\u0026#34;window: [%d, %d)\\n\u0026#34;, left, right); /********************/ // 判断左侧窗口是否要收缩 while (window needs shrink) { // d 是将移出窗口的字符 char d = s[left]; // 左移窗口 left++; // 进行窗口内数据的一系列更新 ... } } } 其中两处 \u0026hellip; 表示的更新窗口数据的地方。\n而且，这两个 \u0026hellip; 处的操作分别是右移和左移窗口更新操作，它们操作是完全对称的。\n套模板要思考下面的问题：\n1、当移动 right 扩大窗口，即加入字符时，应该更新哪些数据？\n2、什么条件下，窗口应该暂停扩大，开始移动 left 缩小窗口？\n3、当移动 left 缩小窗口，即移出字符时，应该更新哪些数据？\n4、我们要的结果应该在扩大窗口时还是缩小窗口时进行更新？\n典型题 3. 无重复字符的最长子串\n76. 最小覆盖子串\n递归 视频链接:递归\n如何编写递归函数 第一步：确定问题\n阶乘：求n的阶乘\n1 2 3 int factorial(int n) { } 斐波那契问题：求第n个斐波那契数\n1 2 3 int fibonacci(int n) { } 汉诺塔问题：将n个盘子从A移动到C\n1 2 3 void hanoi(int n, char A, char B, char C) { } 第二步：解决基准问题（边界条件）\n阶乘：当n为0或1时，阶乘为1\n1 2 3 4 5 int factorial(int n) { if (n == 1) { return 1; } } 斐波那契：当n小于等于2时，答案是1\n1 2 3 4 5 int fibonacci(int n) { if (n \u0026lt;= 2) { return 1; } } 汉诺塔：当n为1时，直接从A移动到C\n1 2 3 4 5 6 void hanoi(int n, char A, char B, char C) { if (n == 1) { cout \u0026lt;\u0026lt; \u0026#34;Move disk 1 from \u0026#34; \u0026lt;\u0026lt; A \u0026lt;\u0026lt; \u0026#34; to \u0026#34; \u0026lt;\u0026lt; C \u0026lt;\u0026lt; endl; return; } } 第三步：拆解问题\n阶乘：n的阶乘等于n乘以(n-1)的阶乘\n1 2 3 4 5 6 int factorial(int n) { if (n == 1) { return 1; } return n * factorial(n - 1); } 斐波那契：第n个斐波那契数等于第n-1和第n-2个斐波那契数之和\n1 2 3 4 5 6 int fibonacci(int n) { if (n \u0026lt;= 2) { return 1; } return fibonacci(n - 1) + fibonacci(n - 2); } 汉诺塔：将n-1个盘子从A移动到B，将第n个盘子从A移动到C，再将n-1个盘子从B移动到C\n1 2 3 4 5 6 7 8 9 void hanoi(int n, char A, char B, char C) { if (n == 1) { cout \u0026lt;\u0026lt; \u0026#34;Move disk 1 from \u0026#34; \u0026lt;\u0026lt; A \u0026lt;\u0026lt; \u0026#34; to \u0026#34; \u0026lt;\u0026lt; C \u0026lt;\u0026lt; endl; return; } hanoi(n - 1, A, C, B); cout \u0026lt;\u0026lt; \u0026#34;Move disk \u0026#34; \u0026lt;\u0026lt; n \u0026lt;\u0026lt; \u0026#34; from \u0026#34; \u0026lt;\u0026lt; A \u0026lt;\u0026lt; \u0026#34; to \u0026#34; \u0026lt;\u0026lt; C \u0026lt;\u0026lt; endl; hanoi(n - 1, B, A, C); } 思维小技巧 在编写函数时，可以当系统库中有一个同名函数，能实现你所需要的功能，直接调用即可。\n典型题 138. 随机链表的复制\n哈希表 更多是起到一个辅助\n典型题 1. 两数之和\n236. 二叉树的最近公共祖先\n","date":"2025-01-10T00:00:00Z","image":"https://example.com/post/algorithm/algorithm.jpg","permalink":"https://example.com/post/algorithm/","title":"算法思路"},{"content":"语音生成与感知模型 发音与感知模型 声门 声带之间的间隙称为声门。\n主要功能：产生激励。\n声道 声道指声门至嘴唇的所有发音器官，包括咽喉、口腔和鼻腔。\n主要功能：传输并调制声波。\n声道的形状变化由舌、软腭、唇、牙决定。\n语音生成动作 语音生成可分为两种功能：\n激励：由声门产生的基本声波。 调制：通过声道形状的变化改变声波的频率特性。 语音生成框图 声门 (激励) ➔ 声道 (调制) ➔ 嘴唇 (辐射语音)\n基音频率 由声带张开闭合的周期决定。\n男性：50-250Hz 女性：100-500Hz 浊音与清音 浊音：由声带振动产生，包括所有元音和部分辅音。 清音：不通过声带振动产生，包括另一部分辅音。 语音生成过程 空气从肺部排出形成气流。 冲击声带： 声带紧绷：形成准周期性脉冲空气流，产生浊音。 声带舒展：形成摩擦音或爆破音。 空气流经过声道调制后从口或鼻腔辐射，形成语音。 共振峰 声道是谐振腔，有许多谐振频率，称为共振峰。 共振峰是声道的重要声学特征。\n听觉掩蔽效应 人耳听觉频率范围：20Hz-20kHz。 语音感知强度范围：0-130dB声压级。 掩蔽效应：一个声音的听觉感受性受到同时存在的另一个声音的影响。 语音信号数字模型的组成 语音信号数字模型：激励模型，声道模型，辐射模型 声道模型：声管模型，共振峰模型 共振峰模型：分为级联型，并联型和混合型 数字语音处理 语音信号基本特性 语音信号频率范围：300-3400Hz。 常用采样率：8kHz。 语音预处理 预处理 包括：预加重，端点检测，加窗分帧\n预加重 目的：增强高频分辨率，减少口唇辐射影响。\n短时处理 加窗：窗长一般选取100-200ms。 窗宽较大：平滑作用明显，反映能量变化较小。 窗宽较小：反映细节快变，包络变化不明显。 短时平均能量 用途：\n区分清音与浊音。 区分有声与无声。 语音识别的辅助参数。 短时自相关函数 浊音：具有明显周期性。 清音：无周期性，类似噪声。 倒谱分析 实现：解卷（卷积关系变换为求和关系）\n将语音信号的声门激励信息与声道响应信息分离。 用于提取声道共振特征和基音周期。 倒谱：频谱(Spectrum)的前四个字母倒过来。 共振峰 语音的主要频率成分，携带声音的辨识属性 提取共振峰：共振峰的位置和转变过程 语音端点检测 端点检测法 指从包含语音的一段信息中确定出语音的起始点和结束点。\n双门限比较法 第一级判决： 根据短时能量选较高门限T1，粗判定语音段。 根据背景噪声平均能量确定较低门限T2，精确定位语音段。 第二级判决： 用短时平均过零率，进一步搜索语音段的起止点。 门限T3由背景噪声平均过零率确定。 语音特性与噪声 语音特性 语音是时变、非平稳的随机信号，同时具有短时平稳性。 语音分为清音与浊音。 语音信号可用统计分析描述。 噪声特性 加性噪声：直接叠加在语音信号上。 非加性噪声：需通过变换处理成加性噪声。 噪声分类 周期性噪声：如机械噪声，用功率谱与滤波去除。 冲激噪声（脉冲噪声）：通过幅度阈值检测并消除。 宽带噪声：难以去除，用白化处理或其他方法。 语音干扰噪声：如“鸡尾酒会效应”，通过语音增强算法处理。 语音增强算法分类 根据是否建立模型： 模型算法： 参数方法 统计方法 非模型算法 根据麦克风数量： 单通道语音增强算法 多通道语音增强算法 根据处理域： 时域 频域 巴克域 子空间域 小波域 谱减法优缺点 优点： 无需使用端点检测方法区分语音段和无声段。 算法简单，易于实现。 缺点： 频谱直接相减会导致增强后的语音产生“音乐噪声”。 适用的信噪比范围较窄。 在低信噪比时对语音可懂度损伤较大。 语音识别系统 系统组成 流程图： 预处理：包括预加重、端点检测。 特征提取：获取语音信号特征参数。 训练识别网络：建立模板和模型。 识别方法 基于声道模型与语音知识。 模式匹配方法：如VQ、DTW。 统计模型方法：如HMM。 人工神经网络方法：如深度学习。 语音识别过程 训练过程\n预处理：输入语音经过预处理。 特征提取：提取语音信号的特征。 模板建立：基于提取的特征建立语音模板。 识别过程\n特征比较：将输入语音特征与现有语音模板进行比较。 最优匹配：找出一系列最优匹配的模板。 结果输出：通过查表给出计算机的识别结果。 隐马尔可夫模型 (HMM) 是一个统计模型\n双重随机过程： 短时平稳段统计特征。 段间动态转变特性。 在语音识别的应用 语音识别的困难：对语音的发音速率和声学变化建立模型 HMM通过以下方式解决上述问题：\n状态转移概率：模拟发音速率的变化，反映大脑根据语法和言语需求调整音素参数的过程。 观察输出概率：模拟声学变化，通过依赖状态的输出概率来描述可观测的语音时变序列。 步骤： 信号预处理。 特征提取。 训练HMM。 测试集识别。 参数 N：模型的状态数目 M：观测符号数 A：状态转移概率分布 B：状态的观测符号概率分布 π：初始状态分布 题目 判断题 声门的主要功能是传输并调制声波。 答案：错误 解析： 声门的主要功能是产生激励，而传输并调制声波是声道的功能。 基音频率由声带张开闭合的周期决定，男性的基音频率范围通常为50-250Hz。 答案：正确 解析： 基音频率确实由声带振动周期决定，男性的基音频率范围通常为50-250Hz。 清音是通过声带振动产生的。 答案：错误 解析： 清音不通过声带振动产生，浊音才是通过声带振动产生的。 共振峰是声道的重要声学特征，反映了声道的谐振频率。 答案：正确 解析： 共振峰是声道的谐振频率，是语音的重要声学特征。 预加重的目的是增强低频分辨率。 答案：错误 解析： 预加重的目的是增强高频分辨率，减少口唇辐射的影响。 单选题 声道的形状变化主要由哪些器官决定？ A. 声带 B. 舌、软腭、唇、牙 C. 肺部 D. 鼻腔 答案：B 解析： 声道的形状变化由舌、软腭、唇、牙决定。 以下哪个频率范围是语音信号的常用频率范围？ A. 20Hz-20kHz B. 300-3400Hz C. 50-250Hz D. 100-500Hz 答案：B 解析： 语音信号的常用频率范围是300-3400Hz。 以下哪种噪声属于周期性噪声？ A. 冲激噪声 B. 宽带噪声 C. 机械噪声 D. 语音干扰噪声 答案：C 解析： 周期性噪声如机械噪声，可以通过功率谱与滤波去除。 在语音识别系统中，以下哪一步骤不属于预处理阶段？ A. 预加重 B. 端点检测 C. 特征提取 D. 加窗 答案：C 解析： 特征提取属于特征提取阶段，而不是预处理阶段。 以下哪种方法属于语音识别的统计模型方法？ A. VQ（矢量量化） B. DTW（动态时间规整） C. HMM（隐马尔可夫模型） D. 深度学习 答案：C\n解析： HMM（隐马尔可夫模型）是一种统计模型方法。 简述题 简述语音生成的过程。 答案： 空气从肺部排出形成气流。 气流冲击声带，声带振动产生基本声波（激励）。 声波经过声道（包括咽喉、口腔和鼻腔）的调制，声道的形状变化由舌、软腭、唇、牙等器官决定。 调制后的声波从口或鼻腔辐射出去，形成语音。 什么是听觉掩蔽效应？ 答案： 听觉掩蔽效应是指一个声音的听觉感受性受到同时存在的另一个声音的影响。具体来说，当一个声音（掩蔽声）存在时，另一个声音（被掩蔽声）的听觉阈值会升高，导致被掩蔽声难以被感知。 简述短时处理中加窗的作用。 答案： 加窗的作用是将语音信号分割成短时段进行处理，以便分析语音的短时特性。窗长一般选取100-200ms，窗宽较大时平滑作用明显，反映能量变化较小；窗宽较小时反映细节快变，包络变化不明显。 什么是倒谱分析？它的主要用途是什么？ 答案： 倒谱分析是将语音信号的声门激励信息与声道响应信息分离的一种方法。它的主要用途是提取声道的共振特征和基音周期，从而帮助分析语音的声学特性。 简述语音识别系统的基本组成。 答案： 预处理：包括预加重、端点检测等。 特征提取：获取语音信号的特征参数。 训练识别网络：建立模板和模型。 识别：通过模式匹配、统计模型或人工神经网络等方法进行语音识别。 综合题 请详细描述语音生成与感知模型中的声道和声门的作用，并结合语音生成框图解释语音生成的过程。 答案： 在语音生成与感知模型中，声门和声道是两个关键部分。 声门： 声门是声带之间的间隙，主要功能是产生激励。当空气从肺部排出时，气流通过声门，声带振动产生基本声波，这个声波是语音生成的起点。 声道： 声道指从声门到嘴唇的所有发音器官，包括咽喉、口腔和鼻腔。声道的主要功能是传输并调制声波。声道的形状变化由舌、软腭、唇、牙等器官决定，这些变化会改变声波的频率特性，从而形成不同的语音。 语音生成框图： 声门（激励）：声带振动产生基本声波。 声道（调制）：声波经过声道的调制，声道的形状变化改变声波的频率特性。 嘴唇（辐射语音）：调制后的声波从嘴唇或鼻腔辐射出去，形成最终的语音。 请结合语音信号的短时处理，解释短时平均能量和短时自相关函数在语音分析中的作用。 答案： 短时平均能量： 短时平均能量是语音信号在短时段内的能量平均值。它的主要用途包括： 区分清音与浊音：浊音的能量通常较高，而清音的能量较低。 区分有声与无声：有声段（如元音）的能量较高，而无声段（如停顿）的能量较低。 作为语音识别的辅助参数：短时平均能量可以帮助识别语音的起始和结束点。 短时自相关函数： 短时自相关函数用于分析语音信号的周期性。它的主要用途包括： 区分浊音与清音：浊音具有明显的周期性，自相关函数会显示出周期性的峰值；而清音无周期性，自相关函数类似噪声。 提取基音周期：通过自相关函数的峰值间隔，可以估计浊音的基音周期。 请详细描述语音识别系统中的隐马尔可夫模型（HMM）的基本原理及其在语音识别中的应用。 答案： 隐马尔可夫模型（HMM）的基本原理： HMM是一种统计模型，用于描述由隐藏的马尔可夫链随机生成的观测序列。HMM包含两个随机过程： 隐藏状态序列：表示系统的内部状态，状态之间的转移遵循马尔可夫性质，即当前状态只依赖于前一个状态。 观测序列：每个隐藏状态生成一个观测值，观测值依赖于当前状态。 HMM在语音识别中的应用： 信号预处理：对语音信号进行预加重、加窗等处理。 特征提取：提取语音信号的特征参数，如MFCC（梅尔频率倒谱系数）。 训练HMM：使用训练数据对HMM进行训练，建立语音模板和模型。 测试集识别：使用训练好的HMM对测试语音进行识别，通过计算观测序列的概率来确定最可能的语音类别。 HMM在语音识别中广泛应用，因为它能够很好地处理语音信号的时变性和短时平稳性，并且能够通过统计方法有效地建模语音的动态特性。 ","date":"2025-01-06T00:00:00Z","image":"https://example.com/post/speech-signal/image-2.png","permalink":"https://example.com/post/speech-signal/","title":"语音信号期末复习"},{"content":"我是直接把这些动画效果全放一个css文件了\n创建一个hover-animation.css文件(可自定义)，然后记得在assets/scss/style.scss下添加@import \u0026quot;hover-animation\u0026quot;; 然后在创建的css文件添加以下代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 /* 主页博客卡片 */ .article-list article { transition: transform 0.6s ease; -webkit-font-smoothing: antialiased; will-change: transform; transform-origin: center; \u0026amp;:hover { transform: scale(1.02, 1.02); } } /* 左侧栏选项 */ #main-menu { overflow: visible; li { a { -webkit-font-smoothing: antialiased; will-change: transform; transition: transform 0.6s ease; \u0026amp;:hover { transform: scale(1.1, 1.1); will-change: transform; } } } } /* 归档和链接卡片 */ .article-list--compact { overflow: visible; } .article-list--compact article { transition: transform 0.6s ease; -webkit-font-smoothing: antialiased; will-change: transform; \u0026amp;:hover { transform: scale(1.05,1.05); z-index: 4; } } /* 分类页面 */ .article-list--tile article { transition: 0.6s ease; } .article-list--tile article:hover { transform: scale(1.05, 1.05); will-change: transform; } /* 右侧导航栏 */ // 搜索 .search-form.widget { transition: transform 0.6s ease; } .search-form.widget:hover { transform: scale(1.1, 1.1); will-change: transform; -webkit-font-smoothing: antialiased; } //归档 .widget.archives .widget-archive--list { transition: transform .3s ease; will-change: transform; } .widget.archives .widget-archive--list:hover { transform: scale(1.05, 1.05); } // 标签 .tagCloud .tagCloud-tags a { border-radius: 10px; font-size: 1.4rem; transition: transform .3s ease; } .tagCloud .tagCloud-tags a:hover { transform: scale(1.1, 1.1); will-change: transform; -webkit-font-smoothing: antialiased; } 参数简单介绍:\n1 2 3 4 5 6 7 8 9 10 11 12 // 动画时间 transition: 0.6s ease; // 放大 transform: scale(1.1, 1.1); // 允许超出边框 overflow: visible; // 这个是为了放大别出现字体抖动（但好像没什么效果） will-change: transform; -webkit-font-smoothing: antialiased; ","date":"2025-01-05T00:00:00Z","image":"https://example.com/post/hugo-animation/word.jpg","permalink":"https://example.com/post/hugo-animation/","title":"【Hugo】动画"},{"content":"这个表盘的设计可能还是有些局限，后续会优化一下\nHTML部分 在\u0026rsquo;layouts/partials/widget/\u0026lsquo;文件夹下创建clock.html文件，并添加以下代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;zh-CN\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;时钟表盘\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;styles.css\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;section class=\u0026#34;widget clock\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;widget--clock\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;clock-face\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;digital-clock\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;hand hour-hand\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;hand minute-hand\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;hand second-hand\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;center-dot\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/section\u0026gt; \u0026lt;script\u0026gt; const clockFace = document.querySelector(\u0026#39;.clock-face\u0026#39;); const radius = 83; // 刻度圆半径 const center = 88; // 刻度圆中心 for (let i = 0; i \u0026lt; 60; i++) { const angle = i * 6; const radians = (angle * Math.PI) / 180; const x = center + radius * Math.sin(radians); const y = center - radius * Math.cos(radians); const mark = document.createElement(\u0026#39;div\u0026#39;); mark.className = \u0026#39;mark\u0026#39;; mark.style.left = `${x}px`; mark.style.top = `${y}px`; mark.style.transform = `translate(-50%, -50%) rotate(${angle}deg)`; if (i % 15 === 0) { mark.classList.add(\u0026#39;long-mark\u0026#39;); const numberRadius = radius - 15; const numberX = center + numberRadius * Math.sin(radians); const numberY = center - numberRadius * Math.cos(radians); const number = document.createElement(\u0026#39;div\u0026#39;); number.textContent = (i / 5) || 12; number.className = \u0026#39;clock-number\u0026#39;; number.style.left = `${numberX}px`; number.style.top = `${numberY}px`; number.style.transform = `translate(-50%, -50%)`; clockFace.appendChild(number); } else if (i % 5 === 0) { mark.classList.add(\u0026#39;middle-mark\u0026#39;); } else { mark.classList.add(\u0026#39;short-mark\u0026#39;); } clockFace.appendChild(mark); } function updateClock() { const now = new Date(); const hour = now.getHours(); const minute = now.getMinutes(); const second = now.getSeconds(); const hourHand = document.querySelector(\u0026#39;.hour-hand\u0026#39;); const minuteHand = document.querySelector(\u0026#39;.minute-hand\u0026#39;); const secondHand = document.querySelector(\u0026#39;.second-hand\u0026#39;); const hourDeg = (hour % 12) * 30 + (minute / 60) * 30; const minuteDeg = minute * 6 + (second / 60) * 6; const secondDeg = second * 6; hourHand.style.transform = `rotate(${hourDeg}deg)`; minuteHand.style.transform = `rotate(${minuteDeg}deg)`; secondHand.style.transition = second === 0 ? \u0026#39;none\u0026#39; : \u0026#39;transform 0.5s linear\u0026#39;; secondHand.style.transform = `rotate(${secondDeg}deg)`; const digitalClock = document.querySelector(\u0026#39;.digital-clock\u0026#39;); const timeString = `${hour}:${minute.toString().padStart(2, \u0026#39;0\u0026#39;)}`; digitalClock.textContent = timeString; setTimeout(updateClock, 1000); } updateClock(); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; css部分 然后在assets/scss文件夹下创建clock.scss文件，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 /* 时钟容器样式 */ .widget.clock { margin-top: 50px; } /* 时钟表盘样式 */ .clock-face { position: relative; width: 200px; height: 200px; border: 12px solid #858b8b; /* 表盘边框颜色 */ border-radius: 50%; margin: 0 auto; background: var(--card-background); /* 动态背景 */ box-shadow: 0 8px 15px rgba(0, 0, 0, 0.2), inset 0 0 8px rgba(255, 255, 255, 0.8); } /* 刻度线通用样式 */ .mark { position: absolute; width: 2px; background: #535656; border-radius: 2px; transform-origin: center center; } /* 长刻度（小时刻度）样式 */ .long-mark { top: 5px; height: 18px; background: #65656c; /* 长刻度颜色稍深 */ box-shadow: 0 2px 5px rgba(0, 0, 0, 0.3); } /* 数字样式 */ .clock-number { position: absolute; font-size: 12px; /* 字体大小 */ color: #797F7F !important; /* 强制覆盖其他样式 */ text-align: center; font-weight: bold; } /* 中刻度（分钟刻度）样式 */ .middle-mark { height: 10px; background: #7f8686; /* 中刻度颜色稍浅 */ box-shadow: 0 1px 3px rgba(0, 0, 0, 0.2); } /* 短刻度（秒刻度）样式 */ .short-mark { height: 5px; background: #c0baba; /* 短刻度颜色稍浅 */ box-shadow: 0 1px 3px rgba(0, 0, 0, 0.2); } /* 指针通用样式 */ .hand { position: absolute; top: 50%; left: 50%; transform-origin: 50% 100%; /* 旋转中心在底部 */ background: #444444; border-radius: 2px; transition: transform 0.5s cubic-bezier(0.4, 2.3, 0.6, 1); } /* 时针样式 */ .hour-hand { width: 6px; height: 40px; background: #6A4C9C; /* 紫色 */ z-index: 3; box-shadow: 0 2px 5px rgba(0, 0, 0, 0.4); top: calc(50% - 40px); left: calc(50% - 3px); } /* 分针样式 */ .minute-hand { width: 4px; height: 60px; background: #B497BD; /* 浅紫色 */ z-index: 2; box-shadow: 0 2px 5px rgba(0, 0, 0, 0.4); top: calc(50% - 60px); left: calc(50% - 2px); } /* 秒针样式 */ .second-hand { width: 2px; height: 70px; background: #FF69B4; /* 亮粉色 */ z-index: 1; box-shadow: 0 2px 8px rgba(255, 99, 71, 0.6); top: calc(50% - 70px); left: calc(50% - 1px); } /* 中心点样式 */ .center-dot { position: absolute; width: 12px; height: 12px; background: #845EC2; border-radius: 50%; top: 50%; left: 50%; transform: translate(-50%, -50%); box-shadow: 0 2px 5px rgba(0, 0, 0, 0.5), inset 0 0 5px rgba(255, 255, 255, 0.8); z-index: 4; } .digital-clock { position: absolute; /* 绝对定位 */ top: 50%; /* 垂直居中 */ left: 50%; /* 水平居中 */ transform: translate(-50%, -50%); /* 精确居中 */ font-size: 24px; /* 字体大小 */ font-family: Arial, sans-serif; /* 字体 */ color: #7C8181; /* 字体颜色 */ opacity: 0; /* 默认隐藏 */ transition: opacity 0.3s ease; /* 添加过渡效果 */ z-index: 5; } .clock:hover .digital-clock { opacity: 1; /* 鼠标悬停时显示 */ } .clock-face { cursor: pointer; /* 鼠标悬停时显示手型指针 */ } 问题 如果时钟刻度有些错位，就修改html文件的\n1 2 const radius = 83; // 刻度圆半径 const center = 88; // 刻度圆中心 ","date":"2025-01-05T00:00:00Z","image":"https://example.com/post/hugo-clock/clock.jpg","permalink":"https://example.com/post/hugo-clock/","title":"【Hugo】时钟"},{"content":"字体 字体文件放在assets/fonts下，然后在layouts/partials/footer/costom.html(没有就创建)中引入,格式如下\n1 2 3 4 5 6 7 8 9 10 11 \u0026lt;style\u0026gt; @font-face { font-family: \u0026#39;MapleMono2\u0026#39;; src: url(\u0026#39;{{ (resources.Get \u0026#34;font/MapleMono2.ttf\u0026#34;).Permalink }}\u0026#39;) format(\u0026#39;truetype\u0026#39;); } :root { --base-font-family: \u0026#39;MapleMono2\u0026#39;; --code-font-family: \u0026#39;MapleMono2\u0026#39;; } \u0026lt;/style\u0026gt; 注意: 字体文件路径src要有后缀\n自定义分类页面样式 我不喜欢原本默认的归档页面，想把归档和分类分成两个页面，具体操作写在了我的Hugo配置博客Hugo配置(stack主题)\n后续想添加对应的页面样式，就在layouts添加对应的html文件，比如layouts/page/category.html，然后在contents/page/category.md中添加layout: \u0026quot;category\u0026quot;，这样就会使用layouts/page/category.html的样式了\n评论功能 这里使用的giscus配置\n先在github page上打开discussion功能 点击setting，向下滑找到discussion，勾选discussion 2. 下载giscus\ngiscus app\n选择仓库地址 3. 配置hugo\n进入giscus官网 giscus\n按照步骤配置，最后复制代码 五个重要参数：\ndata-repo data-repo-id data-category data-category-id data-mapping 添加到配置文件中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 comments: enabled: true provider: giscus giscus: repo: serenNan/serenNan.github.io repoID: category: Announcements categoryID: mapping: pathname lightTheme: light darkTheme: dark reactionsEnabled: 1 emitMetadata: 0 inputPosition: bottom lang: zh-CN 博客背景 我这里用的是particles动态粒子背景\n配置：\n进入网站自定义配置：particles 唯一需要注意的是有个选项改成window\n配置好后下载文件 将particles.min.js 和 particlesjs-config.json放在assets/background文件夹下\n在layouts/partials/footer/custom.html中添加以下代码： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \u0026lt;div id=\u0026#34;particles-js\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script src=\u0026#34;{{ (resources.Get \u0026#34;background/particles.min.js\u0026#34;).RelPermalink }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; particlesJS.load(\u0026#39;particles-js\u0026#39;, \u0026#39;{{ (resources.Get \u0026#34;background/particlesjs-config.json\u0026#34;).RelPermalink }}\u0026#39;, function() { console.log(\u0026#39;particles.js loaded - callback\u0026#39;); }); \u0026lt;/script\u0026gt; \u0026lt;style\u0026gt; #particles-js { position: fixed; top: 0; left: 0; width: 100%; height: 100%; z-index: -1; } \u0026lt;/style\u0026gt; ","date":"2025-01-02T00:00:00Z","image":"https://example.com/post/hugo-beautify/blog2.jpg","permalink":"https://example.com/post/hugo-beautify/","title":"【Hugo】美化\u0026优化(stack主题)"},{"content":"拷贝构造函数 参考文章 csdn：C++拷贝构造函数\n概述 拷贝构造函数，又称复制构造函数，是一种特殊的构造函数，它由编译器调用来完成一些基于同一类的其他对象的构造及初始化。\n其唯一的形参必须是引用，但并不限制为const，一般普遍的会加上const限制。\n调用拷贝构造函数的情形 一个对象作为函数参数，以值传递的方式传入函数体（函数传参，类类型的值传递） 1 2 3 4 5 6 7 8 9 10 11 12 class Complex { }; void Fun(Complex c1) { } int main() { Complex c1(1,2); Fun(c1); // 这里就调用了默认的拷贝构造函数 } 一个对象作为函数返回值，以值传递的方式从函数返回;（函数的返回类型是类，从局部对象到临时对象的拷贝构造） 1 2 3 4 5 Complex Fun() { Complex c(10,20); return c; // 这里会调用 } 一个对象用于给另外一个对象进行初始化(常称为赋值初始化);（用已有对象去初始化本类的其他对象） 1 2 3 4 5 6 int main() { Complex c1(1,2); Complex c2(c1); // 此处 Complex c3=c1; // 此处 } 浅拷贝与深拷贝 当对象的成员变量中存在指针变量时，用存在的对象初始化新建对象时指针变量一同初始化，但这时调用一般拷贝构造函数（浅拷贝）会使新对象中的指针指向和初始化对象指针指向一致，那么当用来初始化的对象在释放内存时会释放掉指针指向的内存，而当新创建的对象释放时会出现程序错误，以为这个指针指向的内存被释放了两次。因此我们需要手动提供另一种拷贝构造函数（深拷贝）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class MyClass { public: int* data; MyClass(int d) { data = new int(d); // 动态分配内存 } ~MyClass() { delete data; // 释放内存 } MyClass(const MyClass\u0026amp; other) { data = new int(*other.data); // 深拷贝：分配新内存并复制内容 } }; int main() { MyClass original(10); MyClass copy(original); // 调用深拷贝构造函数 return 0; } 虚析构函数 总的来说虚析构函数是为了避免内存泄露，而且是当子类中会有指针成员变量时才会使用得到的。也就说虚析构函数使得在删除指向子类对象的基类指针时可以调用子类的析构函数达到释放子类中堆内存的目的，而防止内存泄露的.\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 #include \u0026lt;iostream\u0026gt; using namespace std; class Fish { public: Fish() { cout \u0026lt;\u0026lt; \u0026#34;Constructed Fish\u0026#34; \u0026lt;\u0026lt; endl; } // 如果这里不是虚析构函数，那么delete pFish时只会调用基类的析构函数，而不会调用子类的析构函数 virtual ~Fish() // virtual destructor! { cout \u0026lt;\u0026lt; \u0026#34;Destroyed Fish\u0026#34; \u0026lt;\u0026lt; endl; } }; class Tuna : public Fish { public: Tuna() { cout \u0026lt;\u0026lt; \u0026#34;Constructed Tuna\u0026#34; \u0026lt;\u0026lt; endl; } ~Tuna() { cout \u0026lt;\u0026lt; \u0026#34;Destroyed Tuna\u0026#34; \u0026lt;\u0026lt; endl; } }; void DeleteFishMemory(Fish *pFish) { delete pFish; } int main() { cout \u0026lt;\u0026lt; \u0026#34;Allocating a Tuna on the free store:\u0026#34; \u0026lt;\u0026lt; endl; Tuna *pTuna = new Tuna; cout \u0026lt;\u0026lt; \u0026#34;Deleting the Tuna: \u0026#34; \u0026lt;\u0026lt; endl; DeleteFishMemory(pTuna); cout \u0026lt;\u0026lt; \u0026#34;Instantiating a Tuna on the stack:\u0026#34; \u0026lt;\u0026lt; endl; Tuna myDinner; cout \u0026lt;\u0026lt; \u0026#34;Automatic destruction as it goes out of scope: \u0026#34; \u0026lt;\u0026lt; endl; return 0; } 常量成员函数 常量对象和非常量对象 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #include \u0026lt;iostream\u0026gt; using namespace std; class MyClass { private: int x; public: MyClass(int n) { x = n; } void setX(int n) // 非常量成员函数 { x = n; } int getX() const // 常量成员函数 { return x; } }; int main() { MyClass obj1(10); // 非常量对象 const MyClass obj2(20); // 常量对象 obj1.setX(30); // 可以修改obj1的数据成员 cout \u0026lt;\u0026lt; \u0026#34;obj1.x = \u0026#34; \u0026lt;\u0026lt; obj1.getX() \u0026lt;\u0026lt; endl; // obj1.x = 30 // obj2.setX(40); // 编译错误，不能修改obj2的数据成员（常量对象不能调用非常量成员函数） cout \u0026lt;\u0026lt; \u0026#34;obj2.x = \u0026#34; \u0026lt;\u0026lt; obj2.getX() \u0026lt;\u0026lt; endl; // obj1.x = 20 return 0; } 常量成员函数 常量成员函数的特点 常量成员函数不会修改类的成员函数，即它们是只读的。因此，常量成员函数不能修改类的数据成员，也不能调用非常量成员函数,因为非常量成员函数可能会修改类的数据成员。 常量成员函数可以被常量对象和非常量对象调用。如果一个对象是常量对象，则只能调用该对象的常量成员函数，而不能调用非常量成员函数。 常量成员函数可以访问类的所有成员变量和常量成员函数。 常量成员函数的作用是保证类的数据成员不被修改，从而提高程序的安全性和可靠性。 常量成员函数通常用于访问类的数据成员，而不是修改它们。 例如：可以使用常量成员函数来实现类的数据成员的读取操作，而使用非常量成员函数来实现类的数据成员的写入操作。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 #include \u0026lt;iostream\u0026gt; using namespace std; class Person { private: string name; int age; public: Person(string n, int a) { name = n; age = a; } string getName() const { return name; } int getAge() const { return age; } void show() const { cout \u0026lt;\u0026lt; \u0026#34;Name: \u0026#34; \u0026lt;\u0026lt; name \u0026lt;\u0026lt; \u0026#34;, Age: \u0026#34; \u0026lt;\u0026lt; age \u0026lt;\u0026lt; endl; } }; int main() { Person p(\u0026#34;Alice\u0026#34;, 20); p.show(); return 0; } 左值和右值 参考文章 csdn：C++ 左值和右值\n左值和右值的定义 左值（loactor value）:存储在内存中、可寻址的数据\n右值（read value）:可以提供数据值的数据（不一定可寻址，例如存储在寄存器中的数据）\n右值引用 左值引用无法引用右值； 常量左值引用可以操作右值，但是无法对右值进行修改； 右值引用可以对右值进行修改； 常量右值引用：引用一个右值，并且不可更改。可以常量左值引用代替。 1 2 3 4 5 6 7 int a = 10; int \u0026amp;b = a; // 左值引用 // int \u0026amp;c = 10; // 错误，左值引用无法操作右值 b = 20; const int \u0026amp;d = 10; // 常量左值引用可以操作右值 int \u0026amp;\u0026amp;e = 20; // 右值引用 e = 25; // 修改右值 因此c++11中引入右值引用\u0026amp;\u0026amp;。\n右值引用使用场景 拷贝构造函数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #include \u0026lt;iostream\u0026gt; using namespace std; class demo { public: // 构造函数 demo() : num(new int(0)) { cout \u0026lt;\u0026lt; \u0026#34;construct!\u0026#34; \u0026lt;\u0026lt; endl; } // 拷贝构造函数（深拷贝） demo(const demo \u0026amp;d) : num(new int(*d.num)) { cout \u0026lt;\u0026lt; \u0026#34;copy construct!\u0026#34; \u0026lt;\u0026lt; endl; } ~demo() { cout \u0026lt;\u0026lt; \u0026#34;class destruct!\u0026#34; \u0026lt;\u0026lt; endl; } private: int *num; }; demo get_demo() { return demo(); // 返回一个demo对象，是一个右值 } int main() { demo a (get_demo()); // 拷贝构造 return 0; } 输出：\nconstruct! copy construct! copy construct! class destruct! class destruct! 有些编译器可能会优化，只输出一次拷贝构造函数。\n如上所示，demo 类自定义了一个拷贝构造函数。该函数在拷贝 d.num 指针成员时，必须采用深拷贝的方式，即拷贝该指针成员本身的同时，还要拷贝指针指向的内存资源。否则一旦多个对象中的指针成员指向同一块堆空间，这些对象析构时就会对该空间释放多次，这是不允许的。\ndemo a (get_demo()) 的流程：\n执行 get_demo() 函数，demo()调用构造函数生成一个匿名对象 执行 return demo() ，调用拷贝构造函数拷贝匿名对象，作为函数get_demo()的返回值（get_demo()执行完毕，匿名对象会被销毁） 执行 a(get_demo()), 调用拷贝构造函数(此行代码执行完毕，get_demo()的返回值会被析构) 程序结束前，a被析构。 在这个过程中，底层执行了2次深拷贝。如果指针指向的堆空间较大，会大大降低执行的效率。通过移动构造函数可以解决这个问题。\n何时调用拷贝构造函数？（详见：拷贝构造函数）\n移动构造函数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 #include \u0026lt;iostream\u0026gt; using namespace std; class demo { public: // 构造函数 demo() : num(new int(0)) { cout \u0026lt;\u0026lt; \u0026#34;construct!\u0026#34; \u0026lt;\u0026lt; endl; } // 拷贝构造函数（深拷贝） demo(const demo \u0026amp;d) : num(new int(*d.num)) { cout \u0026lt;\u0026lt; \u0026#34;copy construct!\u0026#34; \u0026lt;\u0026lt; endl; } // 移动构造函数 demo(demo \u0026amp;\u0026amp;d) : num(d.num) { d.num = nullptr; cout \u0026lt;\u0026lt; \u0026#34;move construct!\u0026#34; \u0026lt;\u0026lt; endl; } ~demo() { cout \u0026lt;\u0026lt; \u0026#34;class destruct!\u0026#34; \u0026lt;\u0026lt; endl; } private: int *num; }; demo get_demo() { demo temp; // 创建一个局部对象 return temp; // 返回局部对象 } int main() { demo a(get_demo()); // 调用移动构造函数 return 0; } 输出：\nconstruct! move construct! class destruct! class destruct! 使用右值引用类型的参数，指针浅拷贝，右值对象指针置为nullptr, 从而，避免拷贝堆空间，完成初始化。\n当类中同时包含拷贝构造函数和移动构造函数时，如果使用临时对象初始化当前类的对象，编译器会优先调用移动构造函数来完成此操作。只有当类中没有合适的移动构造函数时，编译器才会退而求其次，调用拷贝构造函数。\nstd::move()可以将左值转换为右值，从而使用移动构造。\n1 2 3 4 5 demo get_demo() { demo temp; // 创建一个局部对象 return std::move(temp); // 使用 std::move 触发移动构造函数 } 输出是一样的\nmove函数 参考文章 csdn：C++11中的move函数\n智能指针 参考文章 csdn：C++智能指针\n智能指针概述 是原始指针的封装，会自动分配内存，不需要担心潜在的内存泄露。\n为什么使用智能指针 一句话带过：智能指针就是帮我们C++程序员管理动态分配的内存的，它会帮助我们自动释放new出来的内存，从而避免内存泄漏。\n下面的内存泄露的例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 #include \u0026lt;iostream\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026lt;memory\u0026gt; using namespace std; // 动态分配内存，没有释放就return void memoryLeak1() { string *str = new string(\u0026#34;动态分配内存！\u0026#34;); return; } // 动态分配内存，虽然有些释放内存的代码，但是被半路截胡return了 int memoryLeak2() { string *str = new string(\u0026#34;内存泄露！\u0026#34;); // ...此处省略一万行代码 // 发生某些异常，需要结束函数 if (1) { return -1; } / // 另外，使用try、catch结束函数，也会造成内存泄漏！ / delete str;\t// 虽然写了释放内存的代码，但是遭到函数中段返回，使得指针没有得到释放 return 1; } int main(void) { memoryLeak1(); memoryLeak2(); return 0; } memoryLeak1函数中，new了一个字符串指针，但是没有delete就已经return结束函数了，导致内存没有被释放，内存泄露！ memoryLeak2函数中，new了一个字符串指针，虽然在函数末尾有些释放内存的代码delete str，但是在delete之前就已经return了，所以内存也没有被释放，内存泄露！\n使用指针，我们没有释放，就会造成内存泄露。但是我们使用普通对象却不会。\n而智能指针本质是对一个普通指针的封装，利用有生命周期的对象自动释放的特性，来实现内存的自动管理。\nauto_ptr auto_ptr 是c++ 98定义的智能指针模板，其定义了管理指针的对象，可以将new获得（直接或间接）的地址赋给这种对象。当对象过期时，其析构函数将使用delete来释放内存！\n用法： 头文件：#include \u0026lt;memory\u0026gt; 用法： auto_ptr\u0026lt;类型\u0026gt; 变量名(new 类型)\n例如：\n1 2 3 auto_ptr\u0026lt; string \u0026gt; str(new string(“我要成为大牛~ 变得很牛逼！”)); auto_ptr\u0026lt;vector\u0026lt; int \u0026gt;\u0026gt; av(new vector\u0026lt; int \u0026gt;()); auto_ptr\u0026lt; int \u0026gt; array(new int[10]); 下面的代码使用new创建一个对象，但是不使用delete，就会发生内存泄露。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 #include \u0026#34;iostream\u0026#34; using namespace std; class Test { public: Test() { cout \u0026lt;\u0026lt; \u0026#34;Test的构造函数...\u0026#34; \u0026lt;\u0026lt; endl; } ~Test() { cout \u0026lt;\u0026lt; \u0026#34;Test的析构函数...\u0026#34; \u0026lt;\u0026lt; endl; } int getDebug() { return this-\u0026gt;debug; } private: int debug = 20; }; int main(void) { Test *test = new Test; cout \u0026lt;\u0026lt; test-\u0026gt;getDebug() \u0026lt;\u0026lt; endl; // delete test; return 0; } 输出：\nTest的构造函数... 要释放内存，就得手动delete，或者使用智能指针\n使用智能指针：\n1 2 3 4 5 6 7 8 9 10 11 int main(void) { // Test *test = new Test; auto_ptr\u0026lt;Test\u0026gt; test(new Test); cout \u0026lt;\u0026lt; \u0026#34;test-\u0026gt;debug：\u0026#34; \u0026lt;\u0026lt; test-\u0026gt;getDebug() \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; \u0026#34;(*test).debug：\u0026#34; \u0026lt;\u0026lt; (*test).getDebug() \u0026lt;\u0026lt; endl; return 0; } 输出：\nTest的构造函数... test-\u0026gt;debug：20 (*test).debug：20 Test的析构函数... 智能指针可以像普通指针一样使用，并且会自动释放内存\n智能指针有三个常用函数：\nget()：获取智能指针管理的指针\n1 2 3 4 5 // 定义智能指针 auto_ptr\u0026lt;Test\u0026gt; test(new Test); Test *tmp = test.get();\t// 获取指针返回 cout \u0026lt;\u0026lt; \u0026#34;tmp-\u0026gt;debug：\u0026#34; \u0026lt;\u0026lt; tmp-\u0026gt;getDebug() \u0026lt;\u0026lt; endl; 但一般不这么使用，因为可以直接使用智能指针操作\nrelease()：释放智能指针管理的指针\n1 2 3 4 5 // 定义智能指针 auto_ptr\u0026lt;Test\u0026gt; test(new Test); Test *tmp2 = test.release();\t// 取消智能指针对动态内存的托管 delete tmp2;\t// 之前分配的内存需要自己手动释放 reset()：重置智能指针管理的指针\n1 2 3 4 5 6 // 定义智能指针 auto_ptr\u0026lt;Test\u0026gt; test(new Test); test.reset();\t// 释放掉智能指针托管的指针内存，并将其置NULL test.reset(new Test());\t// 释放掉智能指针托管的指针内存，并将参数指针取代之 unique_ptr c++11使用unique_ptr替代auto_ptr\nunique_ptr特性：\n基于排他所有权模式：两个指针不能指向同一个资源 无法进行左值unique_ptr复制构造，也无法进行左值复制赋值操作，但允许临时右值赋值构造和赋值 保存指向某个对象的指针，当它本身离开作用域时会自动释放它指向的对象。 在容器中保存指针是安全的 ","date":"2024-12-30T00:00:00Z","image":"https://example.com/post/cpp_study/cpp.jpg","permalink":"https://example.com/post/cpp_study/","title":"C++语法"},{"content":"yaml参数 主框架 1 2 3 4 5 6 baseurl: https://example.com/ languageCode: en-us theme: hugo-theme-stack paginate: 10 title: 个人博客 copyright: serenNan baseurl: 目前是github pages的地址 my-blog\nlanguageCode: 语言代码\ntheme: 主题名称\npaginate: 每页显示的文章数量\ntitle: 网站标题（目前没使用）\ncopyright: 网页最下方显示\n语言 1 2 3 # Theme i18n support # Available values: ar, bn, ca, de, el, en, es, fr, hu, id, it, ja, ko, nl, pt-br, th, uk, zh-cn, zh-hk, zh-tw DefaultContentLanguage: zh-cn DefaultContentLanguage: 默认语言\n网页图标 1 favicon: # e.g.: favicon placed in `static/favicon.ico` of your site folder, then set this field to `/favicon.ico` (`/` is necessary) favicon: 网站图标(将 favicon 放置在站点文件夹的 static/favicon.ico 中，然后将此字段设置为 /favicon.ico（/ 是必需的）。)\n页脚 1 2 3 footer: since: 2024 customText: footer: 页脚\nsince: 年份\ncustomText: 自定义文本\n头像 1 2 3 4 5 6 7 sidebar: emoji: 🐈‍⬛ subtitle: 欢迎来到我的个人博客 avatar: enabled: true local: true src: img/avatar.png sidebar: 侧边栏\navatar: 头像\nsrc: 头像路径\n头像是在 assets/img/avatar.png 中\n文章信息 1 2 3 4 5 6 7 article: math: false toc: true readingTime: true license: enabled: true default: Licensed under CC BY-NC-SA 4.0 article: 文章\nmath: 数学公式\ntoc: 目录\nreadingTime: 阅读时间\nlicense: 许可证\nenabled: 是否启用\ndefault: 默认许可证\n右侧侧边栏 1 2 3 4 5 6 7 8 9 10 11 12 13 14 widgets: homepage: - type: search - type: archives params: limit: 5 - type: categories params: limit: 10 - type: tag-cloud params: limit: 10 page: - type: toc widgets: 小工具 (右边侧边栏)\nsearch: 搜索\narchives: 归档\ncategories: 分类\ntag-cloud: 标签云\ntoc: 目录\n头像下方图标 1 2 3 4 5 6 7 8 9 10 11 12 social: - identifier: github name: GitHub url: https://github.com/CaiJimmy/hugo-theme-stack params: icon: github-2 - identifier: bilibili name: Bilibili url: https://space.bilibili.com/450940909 params: icon: bilibili 头像下方的链接，icon图标放在assets/icons文件夹下，svg后缀。\n左侧侧边栏导航 在content/page文件夹下\nlink链接 在content/page/links.md下 格式:\n1 2 3 4 5 6 7 8 9 links: - title: GitHub description: GitHub is the world\u0026#39;s largest software development platform. website: https://github.com image: https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png - title: TypeScript description: TypeScript is a typed superset of JavaScript that compiles to plain JavaScript. website: https://www.typescriptlang.org image: ts-logo-128.jpg 自定义分类页面 我这里是自定义的分类页面，将归档和分类分开\n在content/categories.md下，每有一个分类就创建一个文件夹，文件夹下放_index.md文件，格式如下：\n注意：是_index.md要加个_\n1 2 3 4 5 6 7 8 title: \u0026#34;文档\u0026#34; date: 2020-03-14T15:40:24+06:00 description : \u0026#34;文档分类\u0026#34; slug: \u0026#34;document\u0026#34; image: 猫.png style: background: \u0026#34;#2a9d8f\u0026#34; color: \u0026#34;#fff\u0026#34; 如果想完善页面，可以在layouts/page/categories.html添加下面的内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 {{ define \u0026#34;body-class\u0026#34; }}template-categories{{ end }} {{ define \u0026#34;main\u0026#34; }} \u0026lt;header\u0026gt; {{- $taxonomy := $.Site.GetPage \u0026#34;taxonomyTerm\u0026#34; \u0026#34;categories\u0026#34; -}} {{- $terms := $taxonomy.Pages -}} {{ if $terms }} \u0026lt;h1 class=\u0026#34;section-title\u0026#34;\u0026gt;分类\u0026lt;/h1\u0026gt; \u0026lt;!-- 这里是标题 --\u0026gt; \u0026lt;div class=\u0026#34;subsection-list\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;article-list--tile\u0026#34; style=\u0026#34;display: flex; flex-direction: column;\u0026#34;\u0026gt; {{ range $terms }} \u0026lt;div class=\u0026#34;category-group\u0026#34; style=\u0026#34;flex: 1 1 auto; margin: 10px;\u0026#34;\u0026gt; \u0026lt;h3 class=\u0026#34;category-title\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt; \u0026lt;/h3\u0026gt; \u0026lt;div class=\u0026#34;article-list--horizontal\u0026#34; style=\u0026#34;display: flex; overflow-x: auto;\u0026#34;\u0026gt; {{ $articles := where .Site.RegularPages \u0026#34;Params.categories\u0026#34; \u0026#34;intersect\u0026#34; (slice .Title) }} {{ range $articles }} \u0026lt;div class=\u0026#34;article-tile\u0026#34; style=\u0026#34;flex: 0 0 auto; margin: 5px;\u0026#34;\u0026gt; {{ partial \u0026#34;article-list/tile\u0026#34; (dict \u0026#34;context\u0026#34; . \u0026#34;size\u0026#34; \u0026#34;250x150\u0026#34; \u0026#34;Type\u0026#34; \u0026#34;taxonomy\u0026#34;) }} \u0026lt;/div\u0026gt; {{ end }} \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {{ end }} \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {{ end }} \u0026lt;/header\u0026gt; {{ partialCached \u0026#34;footer/footer\u0026#34; . }} {{ end }} 文章 在content/post文件夹下，正文就用markdown格式\n主页blog显示 1 2 3 4 5 6 7 8 title: Chinese Test description: 这是一个副标题 date: 2020-09-09 slug: test-chinese # url显示 image: helena-hertz-wWZzXlDpMog-unsplash.jpg categories: - Test - 测试 创建时间\u0026amp;更新时间 1 2 3 4 5 6 7 8 # 更新时间：优先读取git时间 -\u0026gt; git时间不存在，就读取本地文件修改时间 frontmatter: lastmod: - :git - :fileModTime # 允许获取Git信息\tenableGitInfo: true 在部署文件.github/workflows/deploy.yaml 添加：\n1 2 3 4 5 6 - name: Git Configuration run: | git config --global core.quotePath false git config --global core.autocrlf false git config --global core.safecrlf true git config --global core.ignorecase false 注意缩进要对\nstack默认显示在文章最后面，如果想在主页面的博客文章显示，在layouts/partials/article/components/details.html添加：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 \u0026lt;!-- 创建时间\u0026amp;阅读时长 --\u0026gt; \u0026lt;footer class=\u0026#34;article-time\u0026#34;\u0026gt; {{ if $showDate }} \u0026lt;div\u0026gt; {{ partial \u0026#34;helper/icon\u0026#34; \u0026#34;date\u0026#34; }} \u0026lt;time class=\u0026#34;article-time--published\u0026#34;\u0026gt; {{- .Date | time.Format (or .Site.Params.dateFormat.published \u0026#34;Jan 02, 2006\u0026#34;) -}} \u0026lt;/time\u0026gt; \u0026lt;/div\u0026gt; {{ end }} {{ if $showReadingTime }} \u0026lt;div\u0026gt; {{ partial \u0026#34;helper/icon\u0026#34; \u0026#34;clock\u0026#34; }} \u0026lt;time class=\u0026#34;article-time--reading\u0026#34;\u0026gt; {{ T \u0026#34;article.readingTime\u0026#34; .ReadingTime }} \u0026lt;/time\u0026gt; \u0026lt;/div\u0026gt; {{ end }} {{ if and $showDate (ne .Lastmod .Date) }} \u0026lt;span class=\u0026#34;time-divider\u0026#34;\u0026gt;|\u0026lt;/span\u0026gt; {{ end }} {{- if ne .Lastmod .Date -}} \u0026lt;div class=\u0026#34;article-time--lastmod\u0026#34;\u0026gt; {{ partial \u0026#34;helper/icon\u0026#34; \u0026#34;clock\u0026#34; }} \u0026lt;time\u0026gt; {{ .Lastmod.Format ( or .Site.Params.dateFormat.lastUpdated \u0026#34;Jan 02, 2006 15:04 MST\u0026#34; ) }} \u0026lt;/time\u0026gt; \u0026lt;/div\u0026gt; {{- end -}} 自带的有阅读时长，我没开启\n文章末尾也会显示最后修改时间，想删除就去layouts/partials/article/components/footer.html删掉：\n1 2 3 4 5 6 7 8 {{- if ne .Lastmod .Date -}} \u0026lt;section class=\u0026#34;article-lastmod\u0026#34;\u0026gt; {{ partial \u0026#34;helper/icon\u0026#34; \u0026#34;clock\u0026#34; }} \u0026lt;span\u0026gt; {{ T \u0026#34;article.lastUpdatedOn\u0026#34; }} {{ .Lastmod | time.Format ( or .Site.Params.dateFormat.lastUpdated \u0026#34;Jan 02, 2006 15:04 MST\u0026#34; ) }} \u0026lt;/span\u0026gt; \u0026lt;/section\u0026gt; {{- end -}} ","date":"2024-12-29T00:00:00Z","image":"https://example.com/post/hugo-config/blog1.jpg","permalink":"https://example.com/post/hugo-config/","title":"【Hugo】配置(stack主题)"},{"content":"基础命令 CSDN博主总结常用命令\n获得基础信息，输出Metadata 打开媒体文件，获取Meta信息，关闭媒体文件\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 #include \u0026#34;libavutil/log.h\u0026#34; #include \u0026#34;libavformat/avformat.h\u0026#34; // 传入命令行参数个数 int main(int argc, char **argv) { // 设置日志级别 av_log_set_level(AV_LOG_DEBUG); // 设置日志输出函数 // 检查参数个数 if (argc \u0026lt; 2) { av_log(NULL, AV_LOG_DEBUG, \u0026#34;Usage:%s infileName.\\n\u0026#34;, argv[0]); return -1; } // 获取输入文件名 const char *infileName = argv[1]; // 初始化所有组件 AVFormatContext *pFormatCtx = NULL; // 打开媒体文件 int ret = avformat_open_input(\u0026amp;pFormatCtx, infileName, NULL, NULL); // av_err2str()函数返回错误信息 if (ret != 0) { // av_err2str()函数返回错误信息 av_log(NULL, AV_LOG_DEBUG, \u0026#34;open input file:%s failed: %s\\n\u0026#34;, infileName, av_err2str(ret)); return -1; } // 获取媒体文件信息 av_dump_format(pFormatCtx, 0, infileName, 0); // 关闭媒体文件 avformat_close_input(\u0026amp;pFormatCtx); return 0; } 容器/文件 (Container/File) 定义: 特定格式的多媒体文件，如 .mp4, .flv, .mov 等。 作用: 存储和组织多媒体数据，包括音频、视频、字幕等。 常见格式: MP4: 广泛用于视频存储和流媒体。 FLV: 主要用于Flash视频。 MOV: 苹果公司开发的视频格式。 媒体流 (Stream) 定义: 一段连续的数据，如一段声音数据、一段视频或者一段字幕数据。 特点: 由不同编码器编码。 类型: 音频流: 存储音频数据。 视频流: 存储视频数据。 字幕流: 存储字幕数据。 数据包 (Packet) 定义: 一个媒体流由大量的数据包组成，是压缩后的数据。 作用: 传输和存储媒体数据的基本单位。 特点: 数据包是压缩后的数据，便于传输和存储。 数据帧 (Frame) 定义: 一个数据包由一个或多个数据帧组成，是非压缩数据。 作用: 原始的、未压缩的媒体数据。 类型: I帧 (Intra Frame): 独立帧，不依赖其他帧。 P帧 (Predictive Frame): 依赖前一帧进行预测。 B帧 (Bidirectional Frame): 依赖前后帧进行预测。 编解码器 (Codec) 定义: 编解码器是以帧为单位实现压缩数据和原始数据之间相互转换的工具。 作用: 用于压缩和解压缩媒体数据。 常见编解码器: 视频编解码器: H.264, H.265, VP9 等。 音频编解码器: AAC, MP3, Vorbis 等。 重要结构体 AVFormatContext: 管理整个多媒体文件的格式和结构。 AVStream: 表示媒体文件中的一个单独的媒体流。 AVCodecContext 与 AVCodec: 管理媒体数据的编码和解码过程。 AVPacket: 表示压缩后的媒体数据。 AVFrame: 表示未压缩的原始媒体数据。 解封装-提取aac数据 AAC（Advanced Audio Coding）是一种高级音频编码技术，广泛用于数字音频压缩和传输。它是由MPEG（Moving Picture Experts Group）开发的，旨在提供比MP3更高的音质和更高的压缩效率。AAC通常用于各种音频应用，包括音乐、视频、广播和流媒体服务。\nAAC的主要特点：\n高音质：AAC能够在较低的比特率下提供比MP3更高的音质。 多通道支持：AAC支持多通道音频，包括立体声、5.1环绕声和7.1环绕声。 低延迟：AAC设计用于低延迟应用，适合实时音频传输。 灵活性：AAC支持多种比特率和采样率，适用于不同的应用场景。 1 2 ffmpeg -y -i out.mp4 -vn -acodec copy out.aac ffplay out.aac 流程 操作步骤 函数名 打开媒体文件 avformat_open_input 获取码流信息 avformat_find_stream_info 获取音频流 av_find_best_stream 初始化 packet av_packet_alloc 读取 packet 数据 av_read_frame 释放 packet 数据 av_packet_unref 关闭媒体文件 avformat_close_input 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 #include \u0026#34;libavutil/avutil.h\u0026#34; #include \u0026#34;libavformat/avformat.h\u0026#34; int main(int argc, char *argv[]) { // 设置日志级别 av_log_set_level(AV_LOG_DEBUG); // 如果参数小于3，输出使用方法 if (argc \u0026lt; 3) { // argv[0]是程序名 av_log(NULL, AV_LOG_ERROR, \u0026#34;Usage: %s \u0026lt;input file\u0026gt; \u0026lt;output file\u0026gt;\\n\u0026#34;, argv[0]); return -1; } // 获取命令行的输入音频 const char *inputName = argv[1]; // 获取命令行的输出音频 const char *outputName = argv[2]; av_sdp_create; // 打开输入音频文件 AVFormatContext *inFormatCtx = NULL; // 打开媒体文件，并获取流信息 int ret = avformat_open_input(\u0026amp;inFormatCtx, inputName, NULL, NULL); // 如果打开输入文件失败，返回错误信息 if (ret != 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Could not open input file \u0026#39;%s\u0026#39;\\n\u0026#34;, inputName); return -1; } // 获取码流信息 ret = avformat_find_stream_info(inFormatCtx, NULL); // 如果ret小于0，则打印错误信息 if (ret \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;find stream info failed:%s\\n\u0026#34;, av_err2str(ret)); // 就算获取失败，也要关闭输入文件 avformat_close_input(\u0026amp;inFormatCtx); return -1; } // 如果获取成功，则打印信息 int audioIndex = av_find_best_stream(inFormatCtx, AVMEDIA_TYPE_AUDIO, -1, -1, NULL, 0); if (audioIndex \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Could not find audio stream in the input file\\n\u0026#34;); avformat_close_input(\u0026amp;inFormatCtx); return -1; } if (audioIndex \u0026lt; 0) { // 输出错误信息，表示找不到最佳音频流 av_log(NULL, AV_LOG_ERROR, \u0026#34;find best stream failed, index is %d\\n\u0026#34;, audioIndex); avformat_close_input(\u0026amp;inFormatCtx); return -1; } // 打印音频信息 av_log(NULL, AV_LOG_INFO, \u0026#34;the audio index is %d\\n\u0026#34;, audioIndex); // 初始化AVPacket结构体 AVPacket *packet = av_packet_alloc(); if (!packet) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Could not allocate packet\\n\u0026#34;); avformat_close_input(\u0026amp;inFormatCtx); return -1; } // 存储音频流信息 输出文件 FILE *dest_fp = fopen(outputName, \u0026#34;wb\u0026#34;); if (dest_fp == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;open %s file failed\\n\u0026#34;, outputName); // 就算打不开文件也得关闭音频文件 avformat_close_input(\u0026amp;inFormatCtx); // 释放分配的AVPacket av_packet_free(\u0026amp;packet); return -1; } // 有许多PC数据，所以需要循环读取 while (av_read_frame(inFormatCtx, packet) == 0) { // 检查当前包是否属于音频流 if (packet-\u0026gt;stream_index == audioIndex) { // 将音频数据写入输出文件 fwrite(packet-\u0026gt;data, 1, packet-\u0026gt;size, dest_fp); // 检查写入是否成功 if (ret != packet-\u0026gt;size) { // 如果写入的数据大小不等于包的大小，则输出错误信息 av_log(NULL, AV_LOG_ERROR, \u0026#34;write data failed\\n\u0026#34;); // 关闭输出文件 fclose(dest_fp); // 关闭输入文件 avformat_close_input(\u0026amp;inFormatCtx); // 释放整个结构体 av_packet_free(\u0026amp;packet); return -1; } } // 释放当前包的引用 av_packet_unref(packet); } // 检查输入格式上下文是否已初始化 if (inFormatCtx != NULL) { // 关闭输入文件 avformat_close_input(\u0026amp;inFormatCtx); } // 检查输出文件指针是否已初始化 if (dest_fp != NULL) { // 关闭输出文件 fclose(dest_fp); } if (packet != NULL) { // 释放AVPacket结构体 av_packet_free(\u0026amp;packet); } return 0; } aac音频格式分析 ADTS（Audio Data Transport Stream）和ADIF（Audio Data Interchange Format）是两种用于音频编码的容器格式，主要用于AAC（Advanced Audio Codec）音频编码。它们的主要区别在于数据流的组织方式和使用场景。\nADTS（Audio Data Transport Stream） 定义: ADTS是一种流式传输格式，适用于音频数据的实时传输，如广播、流媒体等。 结构: 每个ADTS帧都包含一个头信息，后面跟着音频数据。头信息中包含了帧的长度、采样率、声道数等信息。 特点: 自包含: 每个ADTS帧都是自包含的，可以独立解码。 流式传输: 适合流式传输，因为每个帧都可以独立处理。 头部信息: 每个帧的头部信息较大，可能会增加一些开销。 ADIF（Audio Data Interchange Format） 定义: ADIF是一种文件格式，适用于音频数据的存储和交换，如音频文件的存储。 结构: ADIF文件包含一个唯一的头信息，后面跟着所有的音频数据。头信息中包含了编码参数、采样率、声道数等信息。 特点: 单一头部: 整个文件只有一个头部信息，减少了冗余。 非流式: 不适合流式传输，因为需要整个文件的头信息才能开始解码。 存储和交换: 适合存储和交换音频数据，因为头部信息只出现一次，减少了文件大小。 总结 ADTS: 适用于流式传输，每个帧自包含，适合实时传输。 ADIF: 适用于文件存储和交换，整个文件只有一个头部信息，适合存储和交换音频数据。 选择哪种格式取决于具体的应用场景：如果需要实时传输音频数据，ADTS是更好的选择；如果需要存储或交换音频文件，ADIF更为合适。\n提取H264视频数据 流程 流程和提取aac文件一样\n操作步骤 函数名 打开媒体文件 avformat_open_input 获取码流信息 avformat_find_stream_info 获取音频流 av_find_best_stream 初始化 packet av_packet_alloc 读取 packet 数据 av_read_frame 释放 packet 数据 av_packet_unref 关闭媒体文件 avformat_close_input 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 #include \u0026lt;libavutil/avutil.h\u0026gt; #include \u0026lt;libavformat/avformat.h\u0026gt; int main(int argc, char **argv) { av_log_set_level(AV_LOG_DEBUG); if (argc \u0026lt; 3) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Usage: %s \u0026lt;input\u0026gt; \u0026lt;output\u0026gt;\\n\u0026#34;, argv[0]); return -1; } const char *inFilename = argv[1]; const char *outFilename = argv[2]; AVFormatContext *inFmtCtx = NULL; int ret = avformat_open_input(\u0026amp;inFmtCtx, inFilename, NULL, NULL); if (ret != 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Open input format failed:%s\\n\u0026#34;, av_err2str(ret)); return -1; } ret = avformat_find_stream_info(inFmtCtx, NULL); if (ret != 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Find stream info failed:%s\\n\u0026#34;, av_err2str(ret)); ret = -1; goto fail; } ret = av_find_best_stream(inFmtCtx, AVMEDIA_TYPE_VIDEO, -1, -1, NULL, 0); if (ret \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Find best stream failed:%s\\n\u0026#34;, av_err2str(ret)); ret = -1; goto fail; } int videoIndex = ret; FILE *dest_fp = fopen(outFilename, \u0026#34;wb\u0026#34;); if (dest_fp == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Open output file failed:%s\\n\u0026#34;, outFilename); ret = -1; goto fail; } AVPacket *packet = av_packet_alloc(); while (av_read_frame(inFmtCtx, packet) == 0) { if (packet-\u0026gt;stream_index == videoIndex) { int writeSize = fwrite(packet-\u0026gt;data, 1, packet-\u0026gt;size, dest_fp); if (writeSize != packet-\u0026gt;size) { // 这里不能释放整个packet，只能释放packet中的data，因为循环之后还会用到packet av_packet_unref(packet); ret = -1; break; } } av_packet_free(\u0026amp;packet); } fclose(dest_fp); fail: if(inFmtCtx != NULL) { avformat_close_input(\u0026amp;inFmtCtx); } if(dest_fp != NULL) { fclose(dest_fp); } return ret; } 成功运行，要用avi格式的视频文件\n如果想提取mp4格式的文件，需要进行以下步骤\nmp4→h264 流程 函数名 描述 av_bsf_get_by_name 根据名称获取比特流过滤器 av_bsf_alloc 分配比特流过滤器上下文 avcodec_parameters_copy 复制编解码器参数 av_bsf_init 初始化比特流过滤器 av_bsf_send_packet 发送数据包到比特流过滤器 av_bsf_receive_packet 从比特流过滤器接收处理后的数据包 av_bsf_free 释放比特流过滤器上下文及相关资源 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 #include \u0026lt;libavutil/avutil.h\u0026gt; #include \u0026lt;libavformat/avformat.h\u0026gt; #include \u0026lt;libavcodec/bsf.h\u0026gt; int main(int argc, char **argv) { av_log_set_level(AV_LOG_DEBUG); if (argc \u0026lt; 3) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Usage: %s \u0026lt;input\u0026gt; \u0026lt;output\u0026gt;\\n\u0026#34;, argv[0]); return -1; } const char *inFilename = argv[1]; const char *outFilename = argv[2]; AVFormatContext *inFmtCtx = NULL; int ret = avformat_open_input(\u0026amp;inFmtCtx, inFilename, NULL, NULL); if (ret != 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Open input format failed:%s\\n\u0026#34;, av_err2str(ret)); return -1; } ret = avformat_find_stream_info(inFmtCtx, NULL); if (ret != 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Find stream info failed:%s\\n\u0026#34;, av_err2str(ret)); ret = -1; goto fail; } ret = av_find_best_stream(inFmtCtx, AVMEDIA_TYPE_VIDEO, -1, -1, NULL, 0); if (ret \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Find best stream failed:%s\\n\u0026#34;, av_err2str(ret)); ret = -1; goto fail; } int videoIndex = ret; FILE *dest_fp = fopen(outFilename, \u0026#34;wb+\u0026#34;); if (dest_fp == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Open output file failed:%s\\n\u0026#34;, outFilename); ret = -1; goto fail; } AVPacket *packet = av_packet_alloc(); const AVBitStreamFilter *bsf = av_bsf_get_by_name(\u0026#34;h264_mp4toannexb\u0026#34;); if(bsf == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;get h264_mp4toannexb bsf failed\\n\u0026#34;); ret = -1; goto fail; } AVBSFContext *bsfCtx = NULL; av_bsf_alloc(bsf, \u0026amp;bsfCtx); avcodec_parameters_copy(bsfCtx-\u0026gt;par_in, inFmtCtx-\u0026gt;streams[videoIndex]-\u0026gt;codecpar); av_bsf_init(bsfCtx); while (av_read_frame(inFmtCtx, packet) == 0) { if (packet-\u0026gt;stream_index == videoIndex) { if(av_bsf_send_packet(bsfCtx, packet) == 0) { while(av_bsf_receive_packet(bsfCtx, packet) == 0) { int writeSize = fwrite(packet-\u0026gt;data, 1, packet-\u0026gt;size, dest_fp); if (writeSize != packet-\u0026gt;size) { // 这里不能释放整个packet，只能释放packet中的data，因为循环之后还会用到packet av_packet_unref(packet); ret = -1; break; } } } } av_packet_free(\u0026amp;packet); } fclose(dest_fp); fail: if(inFmtCtx != NULL) { avformat_close_input(\u0026amp;inFmtCtx); } if(bsfCtx != NULL) { av_bsf_free(\u0026amp;bsfCtx); } if(dest_fp != NULL) { fclose(dest_fp); } return ret; } 转封装-mp4转flv I帧，P帧，B帧 I帧：帧内编码帧（Intra picture），I帧通常是一个GOP的第一帧，经过轻度地压缩，作为随机访问的参考点，可以当成静态图像，I帧压缩可去掉视频的空间冗余信息。\nP帧：前向预测编码帧（predictive frame），通过将图像序列中前面已编码帧的时间冗余信息充分去除来压缩传输数据量的编码图像，也称为预测帧。\nB帧：双向预测内插编码帧，既考虑源图像序列前面的已编码帧，又顾及源图像序列后面的已编码帧之间的时间冗余信息，来压缩传输数据量的编码图像，也称为双向预测帧\nPTS-显示时间戳\nDTS-解码时间戳\n流程 步骤 对应函数 打开输入媒体文件 avformat_open_input 获取输入流信息 avformat_find_stream_info 创建输出流上下文 avformat_alloc_output_context2 创建输出码流的AVStream avformat_new_stream 拷贝编码参数 avcodec_parameters_copy 写入视频文件头 avformat_write_header 读取输入视频流 av_read_frame 计算pts/dts/duration av_rescale_q_rnd/av_rescale_q 写入视频流数据 av_interleaved_write_frame 写入视频文件末尾 av_write_trailer 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 #include \u0026lt;libavutil/avutil.h\u0026gt; #include \u0026lt;libavformat/avformat.h\u0026gt; int main(int argc, char **argv) { av_log_set_level(AV_LOG_DEBUG); if (argc \u0026lt; 3) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Usage: %s \u0026lt;infileName\u0026gt; \u0026lt;outfileName\u0026gt;\\n\u0026#34;, argv[0]); return -1; } const char *inFileName = argv[1]; const char *outFileName = argv[2]; AVFormatContext *inFmtCtx = NULL; int ret = avformat_open_input(\u0026amp;inFmtCtx, inFileName, NULL, NULL); if (ret != 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;open input format failed:%s\\n\u0026#34;, av_err2str(ret)); return -1; } ret = avformat_find_stream_info(inFmtCtx, NULL); if (ret != 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;find input stream failed:%s\\n\u0026#34;, av_err2str(ret)); goto fail; } AVFormatContext *outFmtCtx = NULL; // 分配输出格式上下文 ret = avformat_alloc_output_context2(\u0026amp;outFmtCtx, NULL, NULL, outFileName); if (ret \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;alloc output format failed:%s\\n\u0026#34;, av_err2str(ret)); goto fail; } // 输入文件的流数量 int streamCount = inFmtCtx-\u0026gt;nb_streams; // 分配一个整数数组，用于存储输入流索引到输出流索引的映射关系，并将其初始化为零 int *handleStreamIndexArray = av_malloc_array(streamCount, sizeof(int)); if (handleStreamIndexArray == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;malloc handle stream index array failed\\n\u0026#34;); goto fail; } int streamIndex = 0; // 用于多媒体处理的循环，主要功能是将输入文件中的音视频流复制到输出文件中 for (int i = 0; i \u0026lt; streamCount; i++) { // 获取输入文件的流 AVStream *inStream = inFmtCtx-\u0026gt;streams[i]; // 判断流的类型（视频，音频或字幕） if (inStream-\u0026gt;codecpar-\u0026gt;codec_type != AVMEDIA_TYPE_VIDEO \u0026amp;\u0026amp; inStream-\u0026gt;codecpar-\u0026gt;codec_type != AVMEDIA_TYPE_AUDIO \u0026amp;\u0026amp; inStream-\u0026gt;codecpar-\u0026gt;codec_type != AVMEDIA_TYPE_SUBTITLE) { // 不处理该流 handleStreamIndexArray[i] = -1; continue; } handleStreamIndexArray[i] = streamIndex++; // 创建新的输出流 AVStream *outStream = NULL; // 在输出文件中创建一个新的流 outStream = avformat_new_stream(outFmtCtx, NULL); if (outStream == NULL) { ret = -1; av_log(NULL, AV_LOG_ERROR, \u0026#34;new output stream failed\\n\u0026#34;); goto fail; } // 复制编解码器参数 avcodec_parameters_copy(outStream-\u0026gt;codecpar, inStream-\u0026gt;codecpar); // 设置输出流的编解码器标签为0 outStream-\u0026gt;codecpar-\u0026gt;codec_tag = 0; } // 判断outFmtCtx-\u0026gt;oformat-\u0026gt;flags是否包含AVFMT_NOFILE标志 [\u0026amp;解释（点击跳转）](https://www.notion.so/if-outFmtCtx-oformat-flags-AVFMT_NOFILE-1187c25c79d08036bde1c286d0b3c943?pvs=21) if (!(outFmtCtx-\u0026gt;oformat-\u0026gt;flags \u0026amp; AVFMT_NOFILE)) { // 以写入模式打开 ret = avio_open(\u0026amp;outFmtCtx-\u0026gt;pb, outFileName, AVIO_FLAG_WRITE); if (ret \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;open output file failed:%s\\n\u0026#34;, outFileName); goto fail; } } // 将输出文件的头部信息写入到输出文件中 ret = avformat_write_header(outFmtCtx, NULL); if (ret \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;write header failed:%s\\n\u0026#34;, av_err2str(ret)); goto fail; } AVPacket *packet = av_packet_alloc(); // 读取输入文件的数据包 while (av_read_frame(inFmtCtx, packet) == 0) { if (packet-\u0026gt;stream_index \u0026gt;= streamCount || handleStreamIndexArray[packet-\u0026gt;stream_index == -1]) { av_packet_unref(packet); } // 获取输入输出文件中对应流索引的流 AVStream *inStream = inFmtCtx-\u0026gt;streams[packet-\u0026gt;stream_index]; AVStream *outStream = outFmtCtx-\u0026gt;streams[packet-\u0026gt;stream_index]; packet-\u0026gt;stream_index = handleStreamIndexArray[packet-\u0026gt;stream_index]; packet-\u0026gt;pts = av_rescale_q(packet-\u0026gt;pts, inStream-\u0026gt;time_base, outStream-\u0026gt;time_base); packet-\u0026gt;dts = av_rescale_q(packet-\u0026gt;dts, inStream-\u0026gt;time_base, outStream-\u0026gt;time_base); packet-\u0026gt;duration = av_rescale_q(packet-\u0026gt;duration, inStream-\u0026gt;time_base, outStream-\u0026gt;time_base); // 将数据包的位置设置为-1 packet-\u0026gt;pos = -1; ret = av_interleaved_write_frame(outFmtCtx, packet); if (ret != 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;write interleaved failed:%s\\n\u0026#34;, av_err2str(ret)); goto fail; } av_packet_unref(packet); } ret = av_write_trailer(outFmtCtx); if (ret != 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;write trailer failed :%s\\n\u0026#34;, av_err2str(ret)); } fail: if (inFmtCtx) { avformat_close_input(\u0026amp;inFmtCtx); } if (outFmtCtx \u0026amp;\u0026amp; !(outFmtCtx-\u0026gt;oformat-\u0026gt;flags \u0026amp; AVFMT_NOFILE)) { avio_closep(\u0026amp;outFmtCtx-\u0026gt;pb); } if (outFmtCtx) { avformat_free_context(outFmtCtx); } if (handleStreamIndexArray) { av_freep(\u0026amp;handleStreamIndexArray); } return ret; } 截取封装文件 时间基与时间戳 时间基：时间刻度，表示每个刻度多少秒（就像一把尺子的刻度）\n时间戳：表示占多少个时间刻度，单位不是秒，而是时间刻度（多少多少cm）\n时间基和时间戳相乘就是时间\nPTS：显示时间戳，在什么时候开始显示这一帧数据，转成时间：PTS * 时间基\nDTS：解码时间戳，在什么时候开始解码这一帧数据，转成时间：DTS * 时间基\n流程 截取封装文件处理流程和转封装流程几乎一样，只是多了一个跳转指定时间戳的步骤。以下是详细流程：\n步骤 对应函数 1. 打开输入媒体文件 avformat_open_input 2. 获取输入流信息 avformat_find_stream_info 3. 创建输出流上下文 avformat_alloc_output_context2 4. 创建输出码流的AVStream avformat_new_stream 5. 拷贝编码参数 avcodec_parameters_copy 6. 写入视频文件头 avformat_write_header 7. 读取输入视频流 av_read_frame 8. 跳转指定时间戳 av_seek_frame 9. 计算pts/dts/duration av_rescale_q_rnd/av_rescale_q 10. 写入视频流数据 av_interleaved_write_frame 11. 写入视频文件末尾 av_write_trailer 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 #include \u0026lt;libavutil/avutil.h\u0026gt; #include \u0026lt;libavformat/avformat.h\u0026gt; #include \u0026lt;libavcodec/avcodec.h\u0026gt; int main(int argc, char **argv) { // 设置日志级别 av_log_set_level(AV_LOG_INFO); if (argc \u0026lt; 2) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Usage:%s \u0026lt;infileName\u0026gt;\\n\u0026#34;, argv[0]); } const char *inFileName = argv[1]; // 打开输入文件 AVFormatContext *inFmtCtx = NULL; // 用于存储输入文件的格式信息 avformat_open_input(\u0026amp;inFmtCtx, inFileName, NULL, NULL); // 打开输入文件inFileName，并将格式信息存储在inFmtCtx中 avformat_find_stream_info(inFmtCtx, NULL); // 查找输入文件的流信息，并将流信息存储在inFmtCtx中 av_dump_format(inFmtCtx, 0, inFileName, 0); // 打印输入文件inFileName的格式信息 av_log(NULL, AV_LOG_INFO, \u0026#34;input file duration:%ld us, %lf s \\n\u0026#34;, inFmtCtx-\u0026gt;duration, inFmtCtx-\u0026gt;duration * av_q2d(AV_TIME_BASE_Q)); // 打印输入文件的总时长，单位为微秒和秒 AV_TIME_BASE_Q是ffmpeg内部的时间基，值为{1, AV_TIME_BASE}，AV_TIME_BASE的值为1000000，即1秒 // AVRational是ffmpeg内部的时间基，值为{num, den}，num为分子，den为分母 AVRational videoTimeBase; AVRational audioTimeBase; for (int i = 0; i \u0026lt; inFmtCtx-\u0026gt;nb_streams; i++) // 遍历输入文件中的所有流 { AVStream *inStream = inFmtCtx-\u0026gt;streams[i]; // 获取输入文件中的第i个流 // 分别判断是否为音频或视频流 if (inStream-\u0026gt;codecpar-\u0026gt;codec_type == AVMEDIA_TYPE_VIDEO) { videoTimeBase = inStream-\u0026gt;time_base; av_log(NULL, AV_LOG_INFO, \u0026#34;video timebase:num = %d,den = %d\\n\u0026#34;, videoTimeBase.num, videoTimeBase.den); } else if (inStream-\u0026gt;codecpar-\u0026gt;codec_type == AVMEDIA_TYPE_AUDIO) { audioTimeBase = inStream-\u0026gt;time_base; av_log(NULL, AV_LOG_INFO, \u0026#34;audio timebase:num = %d,den = %d\\n\u0026#34;, audioTimeBase.num, audioTimeBase.den); } } AVPacket *packet = av_packet_alloc(); // 分配一个AVPacket结构体，用于存储解码后的数据 while (av_read_frame(inFmtCtx, packet) \u0026gt;= 0) // 循环读取输入文件中的每个数据包，并将数据包存储在packet中 { AVStream *inStream = inFmtCtx-\u0026gt;streams[packet-\u0026gt;stream_index]; // 获取当前数据包所属的流 av_log(NULL, AV_LOG_INFO, \u0026#34;streamIndex = %d,pts = %ld,ptsTime = %lf,dts = %ld,dtsTime = %lf\\n\u0026#34;, packet-\u0026gt;stream_index, packet-\u0026gt;pts, packet-\u0026gt;pts * av_q2d(inStream-\u0026gt;time_base), packet-\u0026gt;dts, packet-\u0026gt;dts * av_q2d(inStream-\u0026gt;time_base)); // 打印当前数据包的流索引、pts、pts时间、dts、dts时间 } return 0; } 视频解码 如何使用ffmpeg接口对视频解码\nRGB介绍 三原色：RGB色彩模式是工业界的一种颜色标准，是通过对红(R)、绿(G)、蓝(B)三个颜色通道的变化以及它们相互之间的叠加来得到各式各样的颜色的，RGB即是代表红、绿、蓝三个通道的颜色，这个标准几乎包括了人类视力所能感知的所有颜色，是目前运用最广的颜色系统之一。\n显示器：使用RGB三种颜色的发光体作为基本发光单元\n分辨率：手机屏幕分辨率是1280*720，表示屏幕上有1280*720个像素点，每个像素点由RGB三种颜色组成\nRGB格式 调色版：通过编号映射到颜色的一张二维表，如01索引，表示红色 索引格式： RGB1、RGB4、RGB8 是计算机图形学中常见的颜色编码格式，它们代表了不同的颜色深度和存储方式。以下是对这些格式的解释：\nRGB1：\n颜色深度：1位（bit）。 颜色数量：2种颜色（通常是黑色和白色）。 应用场景：常用于早期的单色显示器或简单的图形界面，如文本模式下的显示。 RGB4：\n颜色深度：4位（bit）。 颜色数量：16种颜色。 应用场景：常用于早期的彩色显示器或低分辨率图形界面，如早期的计算机游戏或简单的图形应用程序。 RGB8：\n颜色深度：8位（bit）。 颜色数量：256种颜色。 应用场景：常用于早期的彩色显示器或低分辨率图形界面，如早期的计算机游戏、网页设计中的调色板模式等。 这些格式在现代计算机图形处理中已经较少使用，但在某些特定的应用场景或历史研究中仍然具有参考价值。 像素格式：。。。（后续觉得有必要再补上）\n命令\nffmpeg命令将图片转RGB数据\n1 ffmpeg -i input.png -pix_fmt rgb24 output.rgb 注意输出信息中会输出图片大小，下面的ffplay需要用\n1 Stream #0:0: Video: png, rgba(pc, gbr/bt709/iec61966-2-1), 1920x1200 [SAR 5669:5669 DAR 8:5], 25 fps, 25 tbr, 25 tbn ffplay命令播放RGB数据\n1 ffplay -f output.rgb -pix_fmt rgb24 -s widthxheight output.rgb 其中，width 和 height 是图片的宽度和高度，是必要的信息。\n通过解码，会发现照片内存明显变大，因为RGB格式存储了更多的颜色信息，所以我们需要对照片进行编码\nYUV介绍 YUV 是一种颜色编码系统，常用于视频和图像处理中。Y 代表亮度（Luminance），U 和 V 代表色度（Chrominance）。YUV 格式有多种变体，如 YUV420、YUV422、YUV444 等。\n流程 函数名 描述 av_find_best_stream 在媒体文件中查找最佳流 avcodec_alloc_context3 分配一个编解码器上下文 avcodec_parameters_to_context 复制编解码器参数 avcodec_find_decoder 查找并获取视频解码器 avcodec_open2 打开解码器上下文，并与指定的解码器关联 av_read_frame 读取帧 avcodec_send_packet 发送数据包到解码器 avcodec_receive_frame 从解码器接收帧 输入指令\n1 2 ./demoBin ../video/test.mp4 test.yuv ffplay test.yuv -video_size 720x1280 -pixel_format yuv420p 如果播放的视频乱码，主要是由于width和linesize大小不一样 后续的更改视频格式的时候会解决这个问题\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 #include \u0026#34;libavutil/avutil.h\u0026#34; #include \u0026#34;libavformat/avformat.h\u0026#34; #include \u0026#34;libavcodec/avcodec.h\u0026#34; // 定义一个全局变量，用于记录解码的帧数 int frameCount = 0; // 解码视频帧的函数 int decodeVideo(AVCodecContext *codecCtx, AVPacket *packet, FILE *dest_fp) { // 将数据包发送到解码器 int ret = avcodec_send_packet(codecCtx, packet); if (ret != 0) { // 如果发送失败，记录错误信息 av_log(NULL, AV_LOG_ERROR, \u0026#34;Could not send packet:%s\\n\u0026#34;, av_err2str(ret)); return -1; } // 分配一个AVFrame结构体，用于存储解码后的帧数据 AVFrame *frame = av_frame_alloc(); // 循环接收解码后的帧数据 while (avcodec_receive_frame(codecCtx, frame) == 0) { // 将帧数据写入输出文件 fwrite(frame-\u0026gt;data[0], 1, codecCtx-\u0026gt;width * codecCtx-\u0026gt;height, dest_fp); fwrite(frame-\u0026gt;data[1], 1, codecCtx-\u0026gt;width * codecCtx-\u0026gt;height / 4, dest_fp); fwrite(frame-\u0026gt;data[2], 1, codecCtx-\u0026gt;width * codecCtx-\u0026gt;height / 4, dest_fp); // 增加帧计数 frameCount++; // 记录当前帧数 av_log(NULL, AV_LOG_INFO, \u0026#34;frameCount:%d\\n\u0026#34;, frameCount); } // 如果帧数据不为空，释放帧内存 if (frame) { av_frame_free(\u0026amp;frame); } return 0; } int main(int argc, char **argv) { // 设置日志级别为调试模式 av_log_set_level(AV_LOG_DEBUG); // 检查命令行参数是否正确 if (argc \u0026lt; 3) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Usage: %s \u0026lt;input\u0026gt; \u0026lt;output\u0026gt;\\n\u0026#34;, argv[0]); return -1; } // 获取输入和输出文件名 const char *inFileName = argv[1]; const char *outFileName = argv[2]; // 定义一个AVFormatContext结构体，用于存储输入文件的格式信息 AVFormatContext *inFmtCtx = NULL; // 打开输入文件 int ret = avformat_open_input(\u0026amp;inFmtCtx, inFileName, NULL, NULL); if (ret != 0) { // 如果打开失败，记录错误信息 av_log(NULL, AV_LOG_ERROR, \u0026#34;Could not open input file %s\\n\u0026#34;, inFileName); return -1; } // 获取输入文件的流信息 ret = avformat_find_stream_info(inFmtCtx, NULL); if (ret \u0026lt; 0) { // 如果获取失败，记录错误信息 av_log(NULL, AV_LOG_ERROR, \u0026#34;Could not find stream information:%s\\n\u0026#34;, av_err2str(ret)); goto fail; } // 查找最佳的视频流索引 ret = av_find_best_stream(inFmtCtx, AVMEDIA_TYPE_VIDEO, -1, -1, NULL, 0); if (ret \u0026lt; 0) { // 如果查找失败，记录错误信息 av_log(NULL, AV_LOG_ERROR, \u0026#34;Could not find best stream index:%s\\n\u0026#34;, av_err2str(ret)); goto fail; } // 获取视频流的索引 int videoIndex = ret; // 分配一个AVCodecContext结构体，用于存储解码器上下文信息 AVCodecContext *codecCtx = avcodec_alloc_context3(NULL); if (codecCtx == NULL) { // 如果分配失败，记录错误信息 av_log(NULL, AV_LOG_ERROR, \u0026#34;Could not allocate codec context\\n\u0026#34;); ret = -1; goto fail; } // 将流参数复制到解码器上下文 avcodec_parameters_to_context(codecCtx, inFmtCtx-\u0026gt;streams[videoIndex]-\u0026gt;codecpar); // 查找解码器 const AVCodec *decoder = avcodec_find_decoder(codecCtx-\u0026gt;codec_id); if (decoder == NULL) { // 如果查找失败，记录错误信息 av_log(NULL, AV_LOG_ERROR, \u0026#34;Could not find codec\\n\u0026#34;); ret = -1; goto fail; } // 打开解码器 ret = avcodec_open2(codecCtx, decoder, NULL); if (ret != 0) { // 如果打开失败，记录错误信息 av_log(NULL, AV_LOG_ERROR, \u0026#34;Could not open codec:%s\\n\u0026#34;, av_err2str(ret)); goto fail; } // 打开输出文件 FILE *dest_fp = fopen(outFileName, \u0026#34;wb+\u0026#34;); if (dest_fp == NULL) { // 如果打开失败，记录错误信息 av_log(NULL, AV_LOG_ERROR, \u0026#34;Could not open output file %s\\n\u0026#34;, outFileName); ret = -1; goto fail; } // 分配一个AVPacket结构体，用于存储数据包 AVPacket *packet = av_packet_alloc(); // 分配一个AVFrame结构体，用于存储解码后的帧数据 AVFrame *frame = av_frame_alloc(); // 循环读取输入文件中的数据包 while (av_read_frame(inFmtCtx, packet) \u0026gt;= 0) { // 如果数据包属于视频流ff if (packet-\u0026gt;stream_index == videoIndex) { // 解码视频帧 if (decodeVideo(codecCtx, packet, dest_fp) == -1) { ret = -1; av_packet_unref(packet); goto fail; } // 释放数据包引用 av_packet_unref(packet); } } // 刷新解码器，确保所有帧都被解码 decodeVideo(codecCtx, NULL, dest_fp); fail: // 如果输入文件格式上下文不为空，关闭输入文件 if (inFmtCtx) { avformat_close_input(\u0026amp;inFmtCtx); } // 如果解码器上下文不为空，释放解码器上下文 if (codecCtx) { avcodec_free_context(\u0026amp;codecCtx); } // 如果输出文件指针不为空，关闭输出文件 if (dest_fp) { fclose(dest_fp); } return ret; } 更改视频格式 流程 函数名 描述 av_parse_video_size 解析视频尺寸字符串（如 \u0026ldquo;1920x1080\u0026rdquo;）并返回宽度和高度。 sws_getContext 创建一个 SwsContext，用于图像缩放和格式转换。 av_frame_alloc 分配一个 AVFrame 结构体，用于存储解码后的视频帧。 av_image_get_buffer_size 计算给定图像格式和尺寸所需的缓冲区大小。 av_malloc 分配内存，用于存储图像数据。 av_image_fill_arrays 将图像数据填充到 AVFrame 的缓冲区中，并设置相关的行大小和数据指针。 sws_scale 使用 SwsContext 对图像进行缩放或格式转换。 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 #include \u0026#34;libavutil/avutil.h\u0026#34; #include \u0026#34;libavformat/avformat.h\u0026#34; #include \u0026#34;libavcodec/avcodec.h\u0026#34; #include \u0026#34;libswscale/swscale.h\u0026#34; #include \u0026#34;libavutil/parseutils.h\u0026#34; #include \u0026#34;libavutil/imgutils.h\u0026#34; // 定义一个全局变量，用于记录解码的帧数 int frameCount = 0; // 解码视频帧的函数 int decodeVideo(AVCodecContext *codecCtx, AVPacket *packet, struct SwsContext *swsCtx, int destWidth, int destHeight, AVFrame *destFrame, FILE *dest_fp) { // 将数据包发送到解码器 int ret = avcodec_send_packet(codecCtx, packet); if (ret != 0) { // 如果发送失败，记录错误信息 av_log(NULL, AV_LOG_ERROR, \u0026#34;Could not send packet:%s\\n\u0026#34;, av_err2str(ret)); return -1; } // 分配一个AVFrame结构体，用于存储解码后的帧数据 AVFrame *frame = av_frame_alloc(); // 循环接收解码后的帧数据 while (avcodec_receive_frame(codecCtx, frame) == 0) { sws_scale(swsCtx, (const uint8_t *const*)frame-\u0026gt;data, frame-\u0026gt;linesize, 0, codecCtx-\u0026gt;height, destFrame-\u0026gt;data, destFrame-\u0026gt;linesize); // 将帧数据写入输出文件 fwrite(destFrame-\u0026gt;data[0], 1, destWidth * destHeight, dest_fp); fwrite(destFrame-\u0026gt;data[1], 1, destWidth * destHeight / 4, dest_fp); fwrite(destFrame-\u0026gt;data[2], 1, destWidth * destHeight / 4, dest_fp); // 增加帧计数 frameCount++; // 记录当前帧数 av_log(NULL, AV_LOG_INFO, \u0026#34;frameCount:%d\\n\u0026#34;, frameCount); // 输出宽高信息,linesize0 1 2 av_log(NULL, AV_LOG_INFO, \u0026#34;width:%d,height:%d,linesize0:%d,linesize1:%d,linesize2:%d\\n\u0026#34;, destWidth, destHeight, destFrame-\u0026gt;linesize[0], destFrame-\u0026gt;linesize[1], destFrame-\u0026gt;linesize[2]); } // 如果帧数据不为空，释放帧内存 if (frame) { av_frame_free(\u0026amp;frame); } return 0; } int main(int argc, char **argv) { // 设置日志级别为调试模式 av_log_set_level(AV_LOG_DEBUG); // 检查命令行参数是否正确 if (argc \u0026lt; 4) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Usage: %s \u0026lt;input\u0026gt; \u0026lt;output\u0026gt; \u0026lt;width*height\u0026gt;\\n\u0026#34;, argv[0]); return -1; } // 获取输入和输出文件名 const char *inFileName = argv[1]; const char *outFileName = argv[2]; const char *destVideoSizeString = argv[3]; int destWidth = 0, destHeight = 0; int ret = av_parse_video_size(\u0026amp;destWidth, \u0026amp;destHeight, destVideoSizeString); if (ret \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;invalid video size:%s\\n\u0026#34;, destVideoSizeString); return -1; } av_log(NULL, AV_LOG_INFO, \u0026#34;destWith:%d,destHeight:%d\\n\u0026#34;, destWidth, destHeight); // 定义一个AVFormatContext结构体，用于存储输入文件的格式信息 AVFormatContext *inFmtCtx = NULL; // 打开输入文件 ret = avformat_open_input(\u0026amp;inFmtCtx, inFileName, NULL, NULL); if (ret != 0) { // 如果打开失败，记录错误信息 av_log(NULL, AV_LOG_ERROR, \u0026#34;Could not open input file %s\\n\u0026#34;, inFileName); return -1; } // 获取输入文件的流信息 ret = avformat_find_stream_info(inFmtCtx, NULL); if (ret \u0026lt; 0) { // 如果获取失败，记录错误信息 av_log(NULL, AV_LOG_ERROR, \u0026#34;Could not find stream information:%s\\n\u0026#34;, av_err2str(ret)); goto fail; } // 查找最佳的视频流索引 ret = av_find_best_stream(inFmtCtx, AVMEDIA_TYPE_VIDEO, -1, -1, NULL, 0); if (ret \u0026lt; 0) { // 如果查找失败，记录错误信息 av_log(NULL, AV_LOG_ERROR, \u0026#34;Could not find best stream index:%s\\n\u0026#34;, av_err2str(ret)); goto fail; } // 获取视频流的索引 int videoIndex = ret; // 分配一个AVCodecContext结构体，用于存储解码器上下文信息 AVCodecContext *codecCtx = avcodec_alloc_context3(NULL); if (codecCtx == NULL) { // 如果分配失败，记录错误信息 av_log(NULL, AV_LOG_ERROR, \u0026#34;Could not allocate codec context\\n\u0026#34;); ret = -1; goto fail; } // 将流参数复制到解码器上下文 avcodec_parameters_to_context(codecCtx, inFmtCtx-\u0026gt;streams[videoIndex]-\u0026gt;codecpar); // 查找解码器 const AVCodec *decoder = avcodec_find_decoder(codecCtx-\u0026gt;codec_id); if (decoder == NULL) { // 如果查找失败，记录错误信息 av_log(NULL, AV_LOG_ERROR, \u0026#34;Could not find codec\\n\u0026#34;); ret = -1; goto fail; } // 打开解码器 ret = avcodec_open2(codecCtx, decoder, NULL); if (ret != 0) { // 如果打开失败，记录错误信息 av_log(NULL, AV_LOG_ERROR, \u0026#34;Could not open codec:%s\\n\u0026#34;, av_err2str(ret)); goto fail; } enum AVPixelFormat destPixfmt = codecCtx-\u0026gt;pix_fmt; struct SwsContext *swsCtx = sws_getContext(codecCtx-\u0026gt;width, codecCtx-\u0026gt;height, codecCtx-\u0026gt;pix_fmt, destWidth, destHeight, destPixfmt, SWS_BICUBIC, NULL, NULL, NULL); if (swsCtx == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Could not create SwsContext\\n\u0026#34;); ret = -1; goto fail; } AVFrame *destFrame = av_frame_alloc(); uint8_t *outBuffer = av_malloc(av_image_get_buffer_size(destPixfmt, destWidth, destHeight, 1)); av_image_fill_arrays(destFrame-\u0026gt;data, destFrame-\u0026gt;linesize, outBuffer, destPixfmt, destWidth, destHeight, 1); // 打开输出文件 FILE *dest_fp = fopen(outFileName, \u0026#34;wb+\u0026#34;); if (dest_fp == NULL) { // 如果打开失败，记录错误信息 av_log(NULL, AV_LOG_ERROR, \u0026#34;Could not open output file %s\\n\u0026#34;, outFileName); ret = -1; goto fail; } // 分配一个AVPacket结构体，用于存储数据包 AVPacket *packet = av_packet_alloc(); // 分配一个AVFrame结构体，用于存储解码后的帧数据 AVFrame *frame = av_frame_alloc(); // 循环读取输入文件中的数据包 while (av_read_frame(inFmtCtx, packet) \u0026gt;= 0) { // 如果数据包属于视频流 if (packet-\u0026gt;stream_index == videoIndex) { // 解码视频帧 // if (decodeVideo(codecCtx, packet, dest_fp) == -1) if (decodeVideo(codecCtx, packet, swsCtx, destWidth, destHeight, destFrame, dest_fp) == -1) { ret = -1; av_packet_unref(packet); goto fail; } // 释放数据包引用 av_packet_unref(packet); } } // 刷新解码器，确保所有帧都被解码 // decodeVideo(codecCtx, NULL, dest_fp); decodeVideo(codecCtx, NULL, swsCtx, destWidth, destHeight, destFrame, dest_fp); fail: // 如果输入文件格式上下文不为空，关闭输入文件 if (inFmtCtx) { avformat_close_input(\u0026amp;inFmtCtx); } // 如果解码器上下文不为空，释放解码器上下文 if (codecCtx) { avcodec_free_context(\u0026amp;codecCtx); } // 如果输出文件指针不为空，关闭输出文件 if (dest_fp) { fclose(dest_fp); } if (destFrame) { av_frame_free(\u0026amp;destFrame); } if (outBuffer) { av_free(outBuffer); } return ret; } 解码后的数据存储 解码后的视频数据通常存储在 data[0]、data[1]、data[2] 等数组中。具体来说：\ndata[0]: 存储了 linesize[0] * height 个数据。 data[1] 和 data[2]: 存储了其他平面的数据（如YUV格式中的U和V平面）。 内存对齐和 linesize linesize[0]: 实际上并不等于图像的宽度 width，而是比宽度大。 这种差异是由于内存对齐的需求，以及解码器的CPU和其他优化原因导致的。 sws_scale 函数功能 sws_scale 函数是 FFmpeg 中用于图像缩放和格式转换的核心函数。它主要完成以下功能：\n图像色彩空间转换：\n将图像从一种色彩空间转换为另一种色彩空间，例如从 RGB 转换为 YUV，或者从 YUV420P 转换为 YUV444P。 分辨率缩放：\n调整图像的分辨率，例如将 1920x1080 的图像缩放到 1280x720。 前后图像滤波处理：\n在进行缩放和色彩空间转换时，应用滤波器以平滑图像，减少锯齿和伪影。 BMP文件格式 概念：BMP文件格式，又称为Bitmap（位图）或是DIB（Device-Independent Device，设备无光位图），是Windows操作系统中的标准图像文件格式。由于它可以不作任何变换地保存图像像素域的数据，因此成为我们取得RAW数据的好来源。\n扫描方式：从左到右，从下到上\n文件组成：\n位图文件头（Bitmap File Header）：提供文件的格式，大小等信息 位图信息头（Bitmap Information）：提供图像的尺寸，位平面数，压缩方式，颜色索引等信息。 调色板（Color Palette）：可选，有些位图需要调色板，有些位图，比如真彩色图（24位的BMP）就不需要调色板。 位图数据（Bitmap Data）：图像数据区 文件头结构体：\n1 2 3 4 5 6 7 typedef struct tagBITMAPFILEHEADER { WORD bfType; // 文件类型，必须是0x424D，即字符“BM” DWORD bfSize; // bmp文件大小 WORD bfReserved1; // 保留字 WORD bfReserved2; // 保留字 DWORD bfOffBits; // 实际位图数据的偏移字节数，即前三个部分长度之和 } BITMAPFILEHEADER; 信息头结构体：\n1 2 3 4 5 6 7 8 9 10 11 12 13 typedef struct tagBITMAPINFOHEADER { DWORD biSize; //表示struct tagBITMAPINFOHEADER的长度，设为40 LONG biWidth; //bmp图片宽度 LONG biHeight; //bmp图片高度 WORD biPlanes; //bmp图片平面树，设为1 WORD biBitCount; //bmp图片位数，即1位图，4位图，8位图，24位图等 DWORD biCompression; //bmp图片压缩类型，0表示不压缩 DWORD biSizeImage; //bmp图片数据大小，必须是4的整数倍 LONG biXPelsPerMeter; //bmp图片水平分辨率 LONG biYPelsPerMeter; //bmp图片垂直分辨率 DWORD biClrUsed; //bmp图片实际使用的颜色表中的颜色数 DWORD biClrImportant; //bmp图片对显示有重要影响的颜色索引的数目 } BITMAPINFOHEADER; 视频编码（yuv到h264） 流程 函数名 描述 avcodec_find_encoder 查找编码器 avcodec_alloc_context3 创建编码器上下文 avcodec_open2 打开编码器 av_frame_alloc 分配帧内存 av_image_get_buffer_size 获取图像缓冲区大小 av_image_fill_arrays 填充图像数据数组 avcodec_send_frame 发送帧到编码器 avcodec_receive_packet 从编码器接收数据包 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 #include \u0026#34;libavcodec/avcodec.h\u0026#34; #include \u0026#34;libavutil/avutil.h\u0026#34; #include \u0026#34;libavutil/parseutils.h\u0026#34; #include \u0026lt;libavcodec/codec.h\u0026gt; #include \u0026lt;libavcodec/packet.h\u0026gt; #include \u0026lt;libavutil/frame.h\u0026gt; #include \u0026lt;libavutil/imgutils.h\u0026gt; #include \u0026lt;libavutil/log.h\u0026gt; #include \u0026lt;libavutil/rational.h\u0026gt; #include \u0026lt;time.h\u0026gt; int writePacketCount = 0; int encodeVideo(AVCodecContext *encoderCtx, AVFrame *frame, AVPacket *packet, FILE *dest_fp) { int ret = avcodec_send_frame(encoderCtx, frame); if (ret \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;send frame error:%s\\n\u0026#34;, av_err2str(ret)); return -1; } while (ret \u0026gt;= 0) { avcodec_receive_packet(encoderCtx, packet); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) { return 0; } else if (ret \u0026lt; 0) { av_log(NULL,AV_LOG_ERROR,\u0026#34;encoder frrame failed:%s\\n\u0026#34;,av_err2str(ret)); return -1; } fwrite(packet-\u0026gt;data, 1, packet-\u0026gt;size, dest_fp); writePacketCount++; av_log(NULL,AV_LOG_INFO,\u0026#34;writePacketCount : %d\\n\u0026#34;,writePacketCount); av_packet_unref(packet); } } int main(int argc, char **argv) { av_log_set_level(AV_LOG_INFO); if (argc \u0026lt; 5) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Usage: %s \u0026lt;inFile\u0026gt; \u0026lt;outFile\u0026gt; \u0026lt;encodeName\u0026gt; \u0026lt;width x height\u0026gt;\\n\u0026#34;, argv[0]); return -1; } const char *inFileName = argv[1]; const char *outFileName = argv[2]; const char *encoderName = argv[3]; int width = 0, height = 0; int ret = av_parse_video_size(\u0026amp;width, \u0026amp;height, argv[4]); if (ret \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Invalid size \u0026#39;%s\u0026#39;, must be in the form WxH or a valid size abbreviation\\n\u0026#34;, argv[4]); return -1; } enum AVPixelFormat pixFmt = AV_PIX_FMT_YUV420P; int fps = 30; const AVCodec *encoder = avcodec_find_encoder_by_name(encoderName); if (encoder == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;find encoder %s failed\\n\u0026#34;, encoderName); return -1; } AVCodecContext *encoderCtx = avcodec_alloc_context3(encoder); if (encoderCtx == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;alloc encoder context failed!\\n\u0026#34;); return -1; } encoderCtx-\u0026gt;codec_type = AVMEDIA_TYPE_VIDEO; encoderCtx-\u0026gt;pix_fmt = pixFmt; encoderCtx-\u0026gt;width = width; encoderCtx-\u0026gt;height = height; encoderCtx-\u0026gt;time_base = (AVRational){1, fps}; encoderCtx-\u0026gt;bit_rate = 4096000; encoderCtx-\u0026gt;max_b_frames = 0; encoderCtx-\u0026gt;gop_size = 10; ret = avcodec_open2(encoderCtx, encoder, NULL); if (ret \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;open encoder failed! %s\\n\u0026#34;, av_err2str(ret)); goto end; } FILE *src_fp = fopen(inFileName, \u0026#34;rb\u0026#34;); if (src_fp == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;open infilename error\u0026#34;); ret = -1; goto end; } FILE *dest_fp = fopen(outFileName, \u0026#34;wb\u0026#34;); if (dest_fp == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;open outfilename error\u0026#34;); ret = -1; goto end; } AVFrame *frame = av_frame_alloc(); int frameSize = av_image_get_buffer_size(pixFmt, width, height, 1); uint8_t *frameBuffer = av_malloc(frameSize); av_image_fill_arrays(frame-\u0026gt;data, frame-\u0026gt;linesize, frameBuffer, pixFmt, width, height, 1); int pictureSize = width * height; AVPacket *packet = av_packet_alloc(); int readFrameCount = 0; while (fread(frameBuffer, 1, pictureSize * 3 / 2, src_fp) == pictureSize * 3 / 2) { // Y 1 U 1/4 V 1/4 frame-\u0026gt;data[0] = frameBuffer; frame-\u0026gt;data[1] = frameBuffer + pictureSize; frame-\u0026gt;data[2] = frameBuffer + pictureSize + pictureSize / 4; readFrameCount++; av_log(NULL, AV_LOG_INFO, \u0026#34;readFrameCount: %d\\n\u0026#34;, readFrameCount); encodeVideo(encoderCtx, frame, packet, dest_fp); } end: if (encoderCtx) { avcodec_free_context(\u0026amp;encoderCtx); } if (src_fp) { fclose(src_fp); } if (dest_fp) { fclose(dest_fp); } if (frameBuffer) { av_freep(\u0026amp;frameBuffer); } return ret; } 音频解码 PCM介绍 PCM（Pulse Code Modulation）是一种用于数字音频的标准编码格式。它通过将模拟音频信号转换为数字信号来表示音频数据。PCM 编码的基本原理是将模拟音频信号在时间上进行采样，并将每个采样点的幅度值量化为离散的数字值。\n核心过程：采样-\u0026gt;量化-\u0026gt;编码\nPCM关键要素 采样率（Sample Rate）：每秒采样的次数，常见的采样率有 44.1 kHz、48 kHz 等。 量化格式（Sample Format）：每个采样点的位数，常见的量化格式有 16 位、24 位等。 声道数（Channels）：音频信号的声道数，如单声道、立体声等。 PCM数据格式 存储格式\n双声道：采样数据按LRLR方式存储，即左声道和右声道交替存储，存储的时候与字节序有关。 单声道：采样数据按时间顺序存储（有时也会采用LRLR方式，但另一个声道数据为0）。 存储格式分为Packed和Planner两种，对于双通道音频，Packed为两个声道的数据交错存储;Planner为两个声道的数据分开存储。\nPacked：LRLRLR Planner：LLLRRR ffmpeg音频解码后的数据存放在AVFrame结构体中：\nPacked格式下，frame.data[0]存放所有声道的数据。 Planner格式下，frame.data[i]存放第i个声道的数据。 左声道data[0]:LLLL\u0026hellip; 右声道data[1]:RRRR\u0026hellip; Planner模式是ffmpeg内部存储模式，实际使用的音频文件都是Packed模式。\nPCM计算 大小计算：以CD的音质为例：量化格式为16比特（2字节），采样率为44100，声道数为2。 比特率为：16 * 44100 * 2 = 1378.125 kbps 每秒存储空间：1378.125 * 60/8/1024 = 10.09MB ffmpeg提取pcm数据命令： 1 ffmpeg -i input.aac -ar 48000 -ac 2 -f s16le output.pcm ffplay播放pcm数据命令： 1 ffplay -ar 48000 -ac 2 -f s16le output.pcm 通过上述指令播放不成功的话，可以尝试转换PCM文件\n1 2 ffmpeg -f s16le -ar 48000 -ac 2 -i output.pcm output_stereo.wav ffplay output_stereo.wav 流程 函数名 描述 avformat_open_input() 打开输入文件或流并读取头部信息。 avformat_find_stream_info() 读取一些数据包以获取流信息。 av_find_best_stream() 查找最佳流（音频、视频或字幕）。 avcodec_alloc_context3() 分配解码器上下文。 avcodec_parameters_to_context() 将流参数复制到解码器上下文中。 avcodec_find_decoder() 查找合适的解码器。 avcodec_open2() 打开解码器。 av_frame_alloc() 分配AVFrame结构体。 av_samples_get_buffer_size() 计算音频缓冲区的大小。 avcodec_fill_audio_frame() 填充音频帧的缓冲区。 av_read_frame() 从输入文件或流中读取数据包。 avcodec_send_packet() 将数据包发送到解码器进行解码。 avcodec_receive_frame() 从解码器接收解码后的帧。 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 #include \u0026#34;libavcodec/avcodec.h\u0026#34; #include \u0026#34;libavformat/avformat.h\u0026#34; #include \u0026#34;libavutil/avutil.h\u0026#34; #include \u0026lt;libavcodec/codec.h\u0026gt; #include \u0026lt;libavcodec/packet.h\u0026gt; #include \u0026lt;time.h\u0026gt; int decodeAudio(AVCodecContext *decoderCtx, AVPacket *packet, AVFrame *frame, FILE *dest_fp) { int ret = avcodec_send_packet(decoderCtx, packet); if (ret \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;send packet to decoder failed: %s\\n\u0026#34;, av_err2str(ret)); return -1; } int channel = 0; while (ret \u0026gt;= 0) { ret = avcodec_receive_frame(decoderCtx, frame); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) { return 0; } else if (ret \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;decode packet failed: %s\\n\u0026#34;, av_err2str(ret)); return -1; } int dataSize = av_get_bytes_per_sample(decoderCtx-\u0026gt;sample_fmt); if (dataSize \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;get bytes per sample failed\\n\u0026#34;); return -1; } // frame fltp 2 /* data[0] L L L L data[1] R R R R --\u0026gt; L R L R L R L R */ for (int i = 0; i \u0026lt; frame-\u0026gt;nb_samples; i++) { for (channel = 0; channel \u0026lt; decoderCtx-\u0026gt;ch_layout.nb_channels; channel++) { fwrite(frame-\u0026gt;data[channel] + dataSize * i, 1, dataSize, dest_fp); } } } return 0; } int main(int argc, char **argv) { av_log_set_level(AV_LOG_DEBUG); if (argc \u0026lt; 3) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Usage: %s \u0026lt;input\u0026gt; \u0026lt;output\u0026gt;\\n\u0026#34;, argv[0]); } const char *inFileName = argv[1]; const char *outFileName = argv[2]; AVFormatContext *inFmtCtx = NULL; int ret = avformat_open_input(\u0026amp;inFmtCtx, inFileName, NULL, NULL); if (ret \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;open %s failed\\n\u0026#34;, inFileName); return -1; } ret = avformat_find_stream_info(inFmtCtx, NULL); if (ret \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;find stream error:%s\\n\u0026#34;, av_err2str(ret)); goto fail; } ret = av_find_best_stream(inFmtCtx, AVMEDIA_TYPE_AUDIO, -1, -1, NULL, 0); if (ret \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;find best stream error:%s\\n\u0026#34;, av_err2str(ret)); goto fail; } int audioStreamIndex = ret; AVCodecContext *decoderCtx = avcodec_alloc_context3(NULL); if (decoderCtx == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;alloc codec context failed\\n\u0026#34;); goto fail; } ret = avcodec_parameters_to_context(decoderCtx, inFmtCtx-\u0026gt;streams[audioStreamIndex]-\u0026gt;codecpar); const AVCodec *decoder = avcodec_find_decoder(decoderCtx-\u0026gt;codec_id); if (decoder == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;find decoder %d failed\\n\u0026#34;, decoderCtx-\u0026gt;codec_id); ret = -1; goto fail; } ret = avcodec_open2(decoderCtx, decoder, NULL); if (ret != 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;open decoder error:%s\\n\u0026#34;, av_err2str(ret)); goto fail; } FILE *dest_fp = fopen(outFileName, \u0026#34;wb\u0026#34;); if (dest_fp == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;open %s failed\\n\u0026#34;, outFileName); ret = -1; goto fail; } AVFrame *frame = av_frame_alloc(); int frameSize = av_samples_get_buffer_size(NULL, decoderCtx-\u0026gt;ch_layout.nb_channels, frame-\u0026gt;nb_samples, decoderCtx-\u0026gt;sample_fmt, 1); uint8_t *frameBuffer = av_malloc(frameSize); avcodec_fill_audio_frame(frame, decoderCtx-\u0026gt;ch_layout.nb_channels, decoderCtx-\u0026gt;sample_fmt, frameBuffer, frameSize, 1); AVPacket *packet = av_packet_alloc(); while (av_read_frame(inFmtCtx, packet) \u0026gt;= 0) { if (packet-\u0026gt;stream_index == audioStreamIndex) { decodeAudio(decoderCtx, packet, frame, dest_fp); } av_packet_unref(packet); } decodeAudio(decoderCtx, NULL, frame, dest_fp); fail: if (inFmtCtx) { avformat_close_input(\u0026amp;inFmtCtx); } if (decoderCtx) { avcodec_free_context(\u0026amp;decoderCtx); } if (frame) { av_frame_free(\u0026amp;frame); } if (frameBuffer) { av_freep(frameBuffer); } if (dest_fp) { fclose(dest_fp); } return ret; } 运行指令\n1 2 3 4 5 ./demoBin ../video/test.aac ../video/test_decode_by_code.pcm ffmpeg -f f32le -ar 44100 -ac 2 -i ../video/test_decode_by_code.pcm ../video/test_decode_by_code_stereo.wav ffplay ../video/test_decode_by_code_stereo.wav 音频编码 流程 函数名 描述 av_frame_alloc 分配一个AVFrame结构体 av_frame_get_buffer 为AVFrame分配缓冲区 avcodec_find_encoder_by_name 根据名称查找编码器 avcodec_alloc_context3 分配编码器上下文 avcodec_open2 打开编码器 avcodec_send_frame 发送帧到编码器 avcodec_receive_packet 从编码器接收编码后的数据包 运行指令\n1 2 3 ffmpeg -ac 2 -ar 44100 -f s16le -i test.pcm -acodec libfdk_aac test1.aac ffplay test1.aac 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 #include \u0026#34;libavcodec/avcodec.h\u0026#34; #include \u0026#34;libavutil/avutil.h\u0026#34; #include \u0026lt;libavcodec/codec.h\u0026gt; #include \u0026lt;libavcodec/packet.h\u0026gt; #include \u0026lt;libavutil/channel_layout.h\u0026gt; #include \u0026lt;libavutil/error.h\u0026gt; #include \u0026lt;libavutil/log.h\u0026gt; #include \u0026lt;libavutil/frame.h\u0026gt; #include \u0026lt;libavutil/samplefmt.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;time.h\u0026gt; int encodeAudio(AVCodecContext *encoderCtx, AVFrame *frame, AVPacket *packet, FILE *dest_fp) { int ret = avcodec_send_frame(encoderCtx, frame); if (ret \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;send frame to encoder failed:%s\\n\u0026#34;, av_err2str(ret)); return -1; } while (ret \u0026gt;= 0) { ret = avcodec_receive_packet(encoderCtx, packet); if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF) { return 0; } else if (ret \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;receive packet from encoder failed:%s\\n\u0026#34;, av_err2str(ret)); return -1; } fwrite(packet-\u0026gt;data, 1, packet-\u0026gt;size, dest_fp); av_packet_unref(packet); } return 0; } int main(int argc, char **argv) { av_log_set_level(AV_LOG_INFO); if (argc \u0026lt; 3) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Usage: %s \u0026lt;input file\u0026gt; \u0026lt;output file\u0026gt;\\n\u0026#34;, argv[0]); return -1; } const char *inFileName = argv[1]; const char *outFileName = argv[2]; AVFrame *frame = av_frame_alloc(); if (!frame) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Could not allocate video frame\\n\u0026#34;); return -1; } frame-\u0026gt;sample_rate = 44100; // 这里代码有些不同 frame-\u0026gt;ch_layout.nb_channels = 2; av_channel_layout_from_mask(\u0026amp;frame-\u0026gt;ch_layout, AV_CH_LAYOUT_STEREO); frame-\u0026gt;format = AV_SAMPLE_FMT_S16; frame-\u0026gt;nb_samples = 1024; av_frame_get_buffer(frame, 0); int ret = 0; const AVCodec *encoder = avcodec_find_encoder_by_name(\u0026#34;libfdk_aac\u0026#34;); if (!encoder) { av_log(NULL, AV_LOG_ERROR, \u0026#34;find encoder failed\\n\u0026#34;); ret = -1; goto end; } AVCodecContext *encoderCtx = avcodec_alloc_context3(encoder); if (!encoderCtx) { av_log(NULL, AV_LOG_ERROR, \u0026#34;alloc encoder context failed\\n\u0026#34;); ret = -1; goto end; } encoderCtx-\u0026gt;sample_fmt = frame-\u0026gt;format; encoderCtx-\u0026gt;sample_rate = frame-\u0026gt;sample_rate; encoderCtx-\u0026gt;ch_layout.nb_channels = frame-\u0026gt;ch_layout.nb_channels; encoderCtx-\u0026gt;ch_layout = frame-\u0026gt;ch_layout; ret = avcodec_open2(encoderCtx, encoder, NULL); if (ret \u0026lt; 0) { av_log(NULL,AV_LOG_ERROR,\u0026#34;open encoder failed:%s\\n\u0026#34;,av_err2str(ret)); ret = -1; goto end; } FILE *src_fp = fopen(inFileName, \u0026#34;rb\u0026#34;); if (src_fp == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;open input file failed\\n\u0026#34;); ret = -1; goto end; } FILE *dest_fp = fopen(outFileName, \u0026#34;wb+\u0026#34;); if (src_fp == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;open output file failed\\n\u0026#34;); ret = -1; goto end; } AVPacket *packet = av_packet_alloc(); while (1) { int readSize = fread(frame-\u0026gt;data[0], 1, frame-\u0026gt;linesize[0], src_fp); if (readSize == 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;finish read infile\\n\u0026#34;); break; } encodeAudio(encoderCtx, frame, packet, dest_fp); } encodeAudio(encoderCtx, NULL, packet, dest_fp); end: if (frame) { av_frame_free(\u0026amp;frame); } if (encoderCtx) { avcodec_free_context(\u0026amp;encoderCtx); } if (src_fp) { fclose(src_fp); } if (dest_fp) { fclose(dest_fp); } return ret; } 指令\n1 2 3 ./demoBin test.pcm aac_by_code.aac ffplay aac_by_code.aac 视频采集 视频采集命令 查看设备列表： 1 ffmpeg -hide_banner -devices 查看dshow支持的参数： 1 ffmpeg -h demuxer=dshow 查看dshow支持的设备： 1 ffmpeg -f dshow -list_devices true -i dummy 一般是Integrated Camera，这是本地摄像头\n采集摄像头画面： 1 ffmpeg -f dshow -i video=\u0026#34;Integrated Camera\u0026#34; ./video/output.mp4 播放摄像头采集画面：\n1 ffplay output.mp4 流程 函数名 描述 avdevice_register_all 注册所有可用的设备 avformat_alloc_context 分配格式上下文 av_dict_set 设置字典选项 av_find_input_format 查找输入格式 avformat_open_input 打开输入文件 avformat_find_stream_info 查找流信息 av_find_best_stream 查找最佳流 avcodec_alloc_context3 分配编解码器上下文 avcode_parameters_to_context 将参数复制到上下文 avcodec_find_decoder 查找解码器 avcodec_open2 打开编解码器 av_read_frame 读取帧 avcode_send_packet 发送数据包 avcodec_receive_frame 接收帧 颜色空间格式转换：\n函数名 描述 sws_getContext 获取缩放上下文 av_frame_alloc 分配帧 av_image_get_buffer_size 获取图像缓冲区大小 av_malloc 分配内存 av_image_fill_arrays 填充图像数组 sws_scale 缩放图像 先用ffmpeg指令试一下视频采集格式，后续代码写的时候要用对应采集的格式。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 #include \u0026#34;libavcodec/avcodec.h\u0026#34; #include \u0026#34;libavdevice/avdevice.h\u0026#34; #include \u0026#34;libavformat/avformat.h\u0026#34; #include \u0026#34;libavutil/avutil.h\u0026#34; #include \u0026#34;libavutil/imgutils.h\u0026#34; #include \u0026#34;libswscale/swscale.h\u0026#34; #include \u0026lt;libavcodec/packet.h\u0026gt; #include \u0026lt;libavutil/dict.h\u0026gt; #include \u0026lt;libavutil/frame.h\u0026gt; #include \u0026lt;libavutil/log.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;time.h\u0026gt; // 显示可用的摄像头设备 // ffmpeg -f dshow -list_devices true -i dummy void dshowListDevices() { const AVInputFormat *inFmt = av_find_input_format(\u0026#34;dshow\u0026#34;); if (inFmt == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;find dshow failed!\\n\u0026#34;); } // 设置参数 AVDictionary *options = NULL; av_dict_set(\u0026amp;options, \u0026#34;list_devices\u0026#34;, \u0026#34;true\u0026#34;, 0); AVFormatContext *inFmtCtx = avformat_alloc_context(); // 第二个参数是URL int ret = avformat_open_input(\u0026amp;inFmtCtx, \u0026#34;video=Integrated Camera\u0026#34;, inFmt, \u0026amp;options); if (ret != 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;open input format failed:%s\\n\u0026#34;, av_err2str(ret)); return; } if (inFmtCtx) { avformat_close_input(\u0026amp;inFmtCtx); avformat_free_context(inFmtCtx); } } void decodeVideo(struct SwsContext *swsCtx, AVCodecContext *decoderCtx, AVFrame *destFrame, AVPacket *packet, FILE *dest_fp) { if (avcodec_send_packet(decoderCtx, packet) == 0) { AVFrame *frame = av_frame_alloc(); while (avcodec_receive_frame(decoderCtx, frame) \u0026gt;= 0) { sws_scale(swsCtx, (const uint8_t *const *)frame-\u0026gt;data, frame-\u0026gt;linesize, 0, decoderCtx-\u0026gt;height, destFrame-\u0026gt;data, destFrame-\u0026gt;linesize); fwrite(destFrame-\u0026gt;data[0], 1, decoderCtx-\u0026gt;width * decoderCtx-\u0026gt;height, dest_fp); fwrite(destFrame-\u0026gt;data[1], 1, decoderCtx-\u0026gt;width * decoderCtx-\u0026gt;height / 4, dest_fp); fwrite(destFrame-\u0026gt;data[2], 1, decoderCtx-\u0026gt;width * decoderCtx-\u0026gt;height / 4, dest_fp); } av_frame_free(\u0026amp;frame); } } int main(int argc, char *argv[]) { av_log_set_level(AV_LOG_INFO); if (argc \u0026lt; 2) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Usage:%s \u0026lt;outFileName\u0026gt; \\n\u0026#34;, argv[0]); return -1; } const char *outFileName = argv[1]; avdevice_register_all(); dshowListDevices(); AVFormatContext *inFmtCtx = avformat_alloc_context(); const AVInputFormat *inFmt = av_find_input_format(\u0026#34;dshow\u0026#34;); if (inFmt == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;open input format failed!\\n\u0026#34;); goto end; } AVDictionary *options = NULL; av_dict_set(\u0026amp;options, \u0026#34;framerate\u0026#34;, \u0026#34;30\u0026#34;, 0); int ret = avformat_open_input(\u0026amp;inFmtCtx, \u0026#34;video=Integrated Camera\u0026#34;, inFmt, \u0026amp;options); if (ret != 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;open input failed:%s\\n\u0026#34;, av_err2str(ret)); goto end; } ret = avformat_find_stream_info(inFmtCtx, NULL); if (ret \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;find stream info failed:%s\\n\u0026#34;, av_err2str(ret)); goto end; } ret = av_find_best_stream(inFmtCtx, AVMEDIA_TYPE_VIDEO, -1, -1, NULL, 0); if (ret \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;find best stream failed:%s\\n\u0026#34;, av_err2str(ret)); goto end; } int videoIndex = ret; // 创建解码器上下文 AVCodecContext *decoderCtx = avcodec_alloc_context3(NULL); ret = avcodec_parameters_to_context(decoderCtx, inFmtCtx-\u0026gt;streams[videoIndex]-\u0026gt;codecpar); if (ret \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;copy parameters to context failed:%s\\n\u0026#34;, av_err2str(ret)); goto end; } const AVCodec *decoder = avcodec_find_decoder(decoderCtx-\u0026gt;codec_id); if (decoder == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;find decoder failed!\\n\u0026#34;); ret = -1; goto end; } ret = avcodec_open2(decoderCtx, decoder, NULL); if (ret != 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;open decoder failed:%s\\n\u0026#34;, av_err2str(ret)); goto end; } AVFrame *destFrame = av_frame_alloc(); enum AVPixelFormat destPixFmt = AV_PIX_FMT_YUV420P; uint8_t *outBuffer = av_malloc(av_image_get_buffer_size(destPixFmt, decoderCtx-\u0026gt;width, decoderCtx-\u0026gt;height, 1)); av_image_fill_arrays(destFrame-\u0026gt;data, destFrame-\u0026gt;linesize, outBuffer, destPixFmt, decoderCtx-\u0026gt;width, decoderCtx-\u0026gt;height, 1); struct SwsContext *swsCtx = sws_getContext(decoderCtx-\u0026gt;coded_width, decoderCtx-\u0026gt;coded_height, decoderCtx-\u0026gt;pix_fmt, decoderCtx-\u0026gt;width, decoderCtx-\u0026gt;height, destPixFmt, 0, NULL, NULL, NULL); if (swsCtx == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;create sws context failed!\\n\u0026#34;); ret = -1; goto end; } FILE *dest_fp = fopen(outFileName, \u0026#34;wb+\u0026#34;); if (dest_fp == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;open out put file %s failed!\\n\u0026#34;, outFileName); ret = -1; goto end; } AVPacket *packet = av_packet_alloc(); while (1) { if (av_read_frame(inFmtCtx, packet) == 0) { if (packet-\u0026gt;stream_index == videoIndex) { decodeVideo(swsCtx, decoderCtx, destFrame, packet, dest_fp); } } av_packet_unref(packet); } decodeVideo(swsCtx,decoderCtx, destFrame,NULL, dest_fp); end: if (inFmtCtx) { avformat_free_context(inFmtCtx); } if (decoderCtx) { avcodec_free_context(\u0026amp;decoderCtx); } if (dest_fp) { fclose(dest_fp); } if (outBuffer) { av_freep(\u0026amp;outBuffer); } return ret; } 音频采集 音频采集命令 采集麦克风声音：\n1 ffmpeg -f dshow -i audio=\u0026#34;阵列麦克风 (AMD Audio Device)\u0026#34; -ar 44100 -f f32le output.pcm 播放麦克风采集：\n1 ffplay -ar 44100 -f f32le output.pcm 流程 函数名 描述 avdevice_register_all 注册所有可用的设备 avformat_alloc_context 分配格式上下文 av_dict_set 设置字典选项 av_find_input_format 查找输入格式 avformat_open_input 打开输入文件 avformat_find_stream_info 查找流信息 av_find_best_stream 查找最佳流 avcodec_alloc_context3 分配编解码器上下文 avcode_parameters_to_context 将参数复制到上下文 avcodec_find_decoder 查找解码器 avcodec_open2 打开编解码器 av_read_frame 读取帧 avcode_send_packet 发送数据包 avcodec_receive_frame 接收帧 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 #include \u0026#34;libavcodec/avcodec.h\u0026#34; #include \u0026#34;libavdevice/avdevice.h\u0026#34; #include \u0026#34;libavformat/avformat.h\u0026#34; #include \u0026#34;libavutil/avutil.h\u0026#34; #include \u0026#34;libavutil/imgutils.h\u0026#34; #include \u0026#34;libswscale/swscale.h\u0026#34; #include \u0026lt;libavcodec/packet.h\u0026gt; #include \u0026lt;libavutil/dict.h\u0026gt; #include \u0026lt;libavutil/frame.h\u0026gt; #include \u0026lt;libavutil/log.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;time.h\u0026gt; // 显示可用的摄像头设备 // ffmpeg -f dshow -list_devices true -i dummy void dshowListDevices() { const AVInputFormat *inFmt = av_find_input_format(\u0026#34;dshow\u0026#34;); if (inFmt == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;find dshow failed!\\n\u0026#34;); } // 设置参数 AVDictionary *options = NULL; av_dict_set(\u0026amp;options, \u0026#34;list_devices\u0026#34;, \u0026#34;true\u0026#34;, 0); AVFormatContext *inFmtCtx = avformat_alloc_context(); // 第二个参数是URL int ret = avformat_open_input(\u0026amp;inFmtCtx, \u0026#34;video=Integrated Camera\u0026#34;, inFmt, \u0026amp;options); if (ret != 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;open input format failed:%s\\n\u0026#34;, av_err2str(ret)); return; } if (inFmtCtx) { avformat_close_input(\u0026amp;inFmtCtx); avformat_free_context(inFmtCtx); } } void decodeAudio(AVCodecContext *decoderCtx, AVPacket *packet, FILE *dest_fp) { if (avcodec_send_packet(decoderCtx, packet) == 0) { AVFrame *frame = av_frame_alloc(); while (avcodec_receive_frame(decoderCtx, frame) \u0026gt;= 0) { fwrite(frame-\u0026gt;data[0], 1, frame-\u0026gt;linesize[0], dest_fp); } av_frame_free(\u0026amp;frame); } } int main(int argc, char *argv[]) { av_log_set_level(AV_LOG_INFO); if (argc \u0026lt; 2) { av_log(NULL, AV_LOG_ERROR, \u0026#34;Usage:%s \u0026lt;outFileName\u0026gt; \\n\u0026#34;, argv[0]); return -1; } const char *outFileName = argv[1]; avdevice_register_all(); dshowListDevices(); AVFormatContext *inFmtCtx = avformat_alloc_context(); const AVInputFormat *inFmt = av_find_input_format(\u0026#34;dshow\u0026#34;); if (inFmt == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;open input format failed!\\n\u0026#34;); goto end; } AVDictionary *options = NULL; // av_dict_set(\u0026amp;options, \u0026#34;framerate\u0026#34;, \u0026#34;30\u0026#34;, 0); int ret = avformat_open_input(\u0026amp;inFmtCtx, \u0026#34;audio=阵列麦克风 (AMD Audio Device)\u0026#34;, inFmt, \u0026amp;options); if (ret != 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;open input failed:%s\\n\u0026#34;, av_err2str(ret)); goto end; } ret = avformat_find_stream_info(inFmtCtx, NULL); if (ret \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;find stream info failed:%s\\n\u0026#34;, av_err2str(ret)); goto end; } ret = av_find_best_stream(inFmtCtx, AVMEDIA_TYPE_AUDIO, -1, -1, NULL, 0); if (ret \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;find best stream failed:%s\\n\u0026#34;, av_err2str(ret)); goto end; } int audioIndex = ret; // 创建解码器上下文 AVCodecContext *decoderCtx = avcodec_alloc_context3(NULL); ret = avcodec_parameters_to_context(decoderCtx, inFmtCtx-\u0026gt;streams[audioIndex]-\u0026gt;codecpar); if (ret \u0026lt; 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;copy parameters to context failed:%s\\n\u0026#34;, av_err2str(ret)); goto end; } const AVCodec *decoder = avcodec_find_decoder(decoderCtx-\u0026gt;codec_id); if (decoder == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;find decoder failed!\\n\u0026#34;); ret = -1; goto end; } ret = avcodec_open2(decoderCtx, decoder, NULL); if (ret != 0) { av_log(NULL, AV_LOG_ERROR, \u0026#34;open decoder failed:%s\\n\u0026#34;, av_err2str(ret)); goto end; } #if 0 AVFrame *destFrame = av_frame_alloc(); enum AVPixelFormat destPixFmt = AV_PIX_FMT_YUV420P; uint8_t *outBuffer = av_malloc(av_image_get_buffer_size(destPixFmt, decoderCtx-\u0026gt;width, decoderCtx-\u0026gt;height, 1)); av_image_fill_arrays(destFrame-\u0026gt;data, destFrame-\u0026gt;linesize, outBuffer, destPixFmt, decoderCtx-\u0026gt;width, decoderCtx-\u0026gt;height, 1); struct SwsContext *swsCtx = sws_getContext(decoderCtx-\u0026gt;coded_width, decoderCtx-\u0026gt;coded_height, decoderCtx-\u0026gt;pix_fmt, decoderCtx-\u0026gt;width, decoderCtx-\u0026gt;height, destPixFmt, 0, NULL, NULL, NULL); if (swsCtx == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;create sws context failed!\\n\u0026#34;); ret = -1; goto end; } #endif FILE *dest_fp = fopen(outFileName, \u0026#34;wb+\u0026#34;); if (dest_fp == NULL) { av_log(NULL, AV_LOG_ERROR, \u0026#34;open out put file %s failed!\\n\u0026#34;, outFileName); ret = -1; goto end; } AVPacket *packet = av_packet_alloc(); while (1) { if (av_read_frame(inFmtCtx, packet) == 0) { if (packet-\u0026gt;stream_index == audioIndex) { // decodeVideo(swsCtx, decoderCtx, destFrame, packet, dest_fp); decodeAudio(decoderCtx, packet, dest_fp); } } av_packet_unref(packet); } // decodeVideo(swsCtx, decoderCtx, destFrame, NULL, dest_fp); decodeAudio(decoderCtx, NULL, dest_fp); end: if (inFmtCtx) { avformat_free_context(inFmtCtx); } if (decoderCtx) { avcodec_free_context(\u0026amp;decoderCtx); } if (dest_fp) { fclose(dest_fp); } return ret; } 采集完后要用指令\n1 ffplay -f s16le -ar 44100 code.pcm 才可以播放，可能是参数的不同\n","date":"2024-12-29T00:00:00Z","image":"https://example.com/post/video_base/player1.jpg","permalink":"https://example.com/post/video_base/","title":"音视频基础"}]